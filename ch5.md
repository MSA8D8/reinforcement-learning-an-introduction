---
marp: true
theme: default
math : MathJax


---
<!-- paginate: true -->
# Chapter 5: Monte Carlo Methods
##### 2023.12.1
##### 佐々木

<!-- <style scoped>
section { font-size: 23px; }
</style> -->

---
# Monte Carlo methodsの導入
- 本章では、価値関数を推定し、最適な方針を発見するための最初の学習法を考える。**前章とは異なり、ここでは環境の完全な知識を前提としない。**
- モンテカルロ法は、実際の、あるいはシミュレートされた環境から得られた、状態、行動、報酬のサンプルシーケンスである経験だけを必要とする。
- モデルは必要であるが、 **動的計画法(DP)に必要なすべての可能な遷移の完全な確率分布ではなく、サンプルの遷移を生成するだけでよい。**

---
# Monte Carlo methods 
モンテカルロという用語 : 操作に重要なランダム要素が含まれるあらゆる推定方法に対して、より広義に使用されることが多い。

ここでは特に、**完全なリターン(= complete return)の平均に基づく方法**(部分的なリターンから学習する方法は次の章で検討)に使用する。

- well-definedされたリターンが得られるように、**ここではモンテカルロ法をエピソード・タスクに対してのみ定義する。**
  - つまり、**経験はエピソードに分割され、どのような行動を選択しても、すべてのエピソードは最終的に終了すると仮定する。**
  - エピソードが終了したときのみ、valueのestimateとactionが変更される。
  - したがって、エピソードごとの意味ではインクリメンタルであるが、ステップバイステップ(オンライン)の意味ではインクリメンタルではない。

---
# Monte Carlo methods 
モンテカルロ法は、
- 各状態とアクションのペアのリターンをサンプリングして平均する。
- つまり、ある状態で行動をとった後のリターンは、同じエピソード内の後の状態でとった行動に依存する。
- よって、すべての行動選択は学習中であるため、先の状態から見て問題は非定常となる。

---
# Monte Carlo for episodic tasks
非定常性を扱うために、第4章で紹介した**GPI(general policy iteration)の考え方をDPに適用する。** 
  - 4章では、MDPの知識から価値関数を計算したが、ここではMDPを用いたサンプルリターンから価値関数を学習する。
  - 価値関数とそれに対応するpolicyは、基本的に同じ方法(GPI)で最適性を達成するためにinteractionする。

DPの章と同様に、
- まず予測問題を考える (the computation of $v_{\pi}$ and $q_{\pi}$ for a fixed arbitrary policy $\pi$)
- 次に、policy imporovement
- 最後に、GPIによる制御問題とその解法。

DPからの各アイデアは、サンプル経験のみが利用可能なモンテカルロ法に拡張される

---
# 5.1 Monte Carlo Prediction
**まず、与えられたpolicyの状態-価値関数を学習するモンテカルロ法を考える。** 

- ある状態($s$)の価値($v$)とは、その状態からの期待リターンであることを思い出してください。

- 経験則に基づいて、この値を推定する明白な方法は、**その状態を訪れた後に観察されたリターンを単純に平均すること。**

- より多くのリターンが観測されれば、平均値は期待値に収束するはずである。

---
# 5.1 Monte Carlo Prediction
**あるポリシー$\pi$に従って$s$を通過すると得られるエピソードの集合$S$が与えられたとき、ポリシーの下での状態$s$の値$v_{\pi}(s)$を推定したいとする。**

あるエピソードにおける状態$s$の各出現は、$s$への訪問(visit)と呼ばれる。もちろん、$s$は同じエピソードで複数回訪問されるかもしれない。あるエピソードで$s$が最初に訪問されたときを、$s$へのfirst-visitと呼ぶことにする。

- first-visitの場合、$v_{\pi}(s)$を$s$への初回訪問後のリターンの平均として推定する
- 毎回訪問する(every-visit)場合は、$s$へのすべての訪問後のリターンを平均する

これら2つのモンテカルロ法は非常によく似ているが、理論的性質は若干異なる。**本章で取り上げるのはFirst-visitの場合である。**

<!-- 
---
<style scoped>
section { font-size: 20px; }
</style>

# First-visit MCのアルゴリズム : estimate $V \approx v_{\pi}$

Input: 
- 評価されるpolicy $\pi$ 

初期化:
- $V (s) \in R$、任意、すべての $s \in S$ に対して
- $Returns(s)$ ← すべての $s \in S$ に対して、空のリスト。

永遠にループする(各エピソードごとに):
- $\pi$に続くエピソードを生成する:$s_{0} , a_{0} , r_{1} , s_{1} , a_{1} , r_{2} ,..., s_{T—1} , a_{T—1} , r_{T}$ 
- $g ← 0$ 
- エピソードの各ステップ、$t = T-1, T-2, ..., 0$に対してループ: 
  - $G ← \gamma G + R_{t+1}$ ($\gamma$は割引率)
  - $S_{t}$が$S_{0}, S_{1} ,..., S_{t—1}$に出現しない限り:
    - $Returns(S_{t})$に$G$を追加する。
    - $V(S_{t}) ← average(Returns(S_{t}))$ -->


---
# Example 5.1: Blackjack 
- 二枚のカードの合計が21に近い方が勝ち。21を超えると負け。21以下ならカードを引くか引かないか (hit or stand) を選択できる。
- 絵札は10, aceは1 or 11になる.
- **The game begins with two cards dealt to both dealer and player.**
  - **One of the dealer’s cards is face up and the other is face down.**
- ディーラーは、17以上になるまでカードを引く。17以上になったら、引かない。21を超えたら負け。21以下なら、プレイヤーと比較して勝ち負けが決まる。
- カードは無限の山札から配られる（つまり、交換される）ので、すでに配られたカードを記録しておく利点はないと仮定する. ← 重要な前提条件(現実とは違う)。

---
# Example 5.1: Blackjack 
ブラックジャックをプレイすることは，当然ながらエピソード有限MDPとして定式化される. 
- Each game of blackjack is an episode. 
- Rewards of +1, -1, and 0 are given for winning, losing, and drawing, respectively. 
- All rewards within a game are zero, and we do not discount ($\gamma$ = 1); 
- therefore these terminal rewards are also the returns. 
- The player’s actions are to hit or to stick. 
- The states depend on the player’s cards and the dealer’s showing card. 


--- 
# Example 5.1: Blackjack 


もし、playerがaceを11として使えるなら、それをusableと定義する。usableなら、hitしたほうが絶対に良い(ace = 1 or 11)。

Thus, the player makes decisions on the basis of three variables: 
 - his current sum(12–21), 
 - the dealer’s one showing card (ace–10), 
 - and whether or not he holds a usable ace. 

This makes for a total of 200 states.

---
# Example 5.1: Blackjack 
プレイヤーが20か21であればstandし，そうでなければhitするというpolicyを考える.

**このポリシーのstate-value関数をモンテカルロ法で求めるには、このpolicyを使ってブラックジャックゲームをたくさんシミュレートし，各状態に続くリターンを平均化する.**

このようにして，図5.1に示す状態値関数の推定値が得られた (教科書参照).

---
# Example 5.1: Blackjack 
**ブラックジャックでは、DP法を適用して価値関数を計算することは容易ではない。**
- 例えば，プレイヤーが14で，standを選んだとする。ディーラーがカードを出したときの関数として，プレイヤーの報酬が+1される確率は?

- DPでは、すべての確率を計算しなければならないが、モンテカルロ法で必要なサンプルの生成は簡単にできる。

- ここで重要な点は、各状態の推定値が独立していることである。DPの場合のように、ある状態の推定値が他の状態の推定値の上に構築されることはない。

---
# 5.2 Estimation of Action Values (modelがない場合)
モデルがあれば、状態値だけでpolicyを決めるのに十分である。DPの章でやったように、一歩先を見て、報酬と次の状態の最良の組み合わせにつながる行動を選ぶだけ。

**しかし、モデルがなければ、状態値だけでは十分ではない。** 
  - その値をpolicy提案に役立てるため、各行動のvalueを明示的に推定する必要がある
  - これを達成するために、**まず、action-valueに対するpolicyの評価問題を考える。**

---
# action-valueに対するpolicy評価の定義
行動に対するpolicy評価の問題 : 
- 状態 $s$ から始めて行動 $a$ をとり、その後policy $\pi$ に従うときの期待リターン$q_{\pi}(s, a)$を推定すること。
- ただし、ここでは状態への訪問ではなく、状態とアクションのペアへの訪問について述べる。
- 状態-行動ペア$(s,a)$は、あるエピソードにおいて、状態$s$が訪問され、その中で行動$a$が取られた場合に、訪問されたという。

唯一の複雑な点は、**多くの状態とアクションのペアが一度も訪問されない可能性があることである。**

---
# 行動に対するpolicy評価の問題
もし平均できるリターンがない場合、モンテカルロ法が使えない。これは深刻な問題である。選択肢を比較するためには、各状態のすべての行動の価値を推定する必要がある。

そこで、エピソードが無限に続く限り、すべての状態アクション・ペアが無限に訪れることを保証する。これを**探索開始の仮定**と呼ぶ(現実的にはありえない仮定であるから、後に取り除く方法が導入される)。

今のところ、探索開始の仮定を維持し、完全なモンテカルロ制御法の提示を完了する。

---
# 5.3 Monte Carlo Control
以上で、モンテカルロ推定から最適なポリシーを近似することについて考える準備ができた。

全体的な考え方は、DPの章と同じパターンの考え方に従って進めることである。

GPIでは、近似的な方針と近似的な価値関数の両方を維持する。
- 価値関数は現在のpolicyの価値関数により近くなるように繰り返し変更され、
- policyは現在の価値関数に対して繰り返し改善される。

この2種類の変更を一緒に行うことで、policyと価値関数の両方が最適に近づく。

---
# 5.3 Monte Carlo Control
はじめに、古典的なpolicy反復を考えてみよう。この方法では、policy評価とpolicy反復の完全なステップを交互に実行する。任意のpolicy $\pi_{0}$ から始まり、最適なpolicyと最適な行動価値関数で終わる:

$$
\pi_{0}\stackrel{E}{\longrightarrow}q_{\pi_{0}}\stackrel{I}{\longrightarrow}\pi_{1}\stackrel{E}{\longrightarrow}q_{\pi_{1}}\stackrel{I}{\longrightarrow}\pi_{2}\cdots
$$

ここで、
- $\stackrel{E}{\longrightarrow}$は完全なpolicy(complete policy)評価を表し、
- $\stackrel{I}{\longrightarrow}$は完全なpolicy improvementを表す。

とりあえず、本当に無限のエピソードを観察し、さらに、エピソードが次のように生成されると仮定しよう。これらの仮定の下で、モンテカルロ法は、任意の$q_{\pi_{k}}$に対して、各$\pi_{k}$を正確に計算する。

---
# 5.3 Monte Carlo Control
ポリシーの改善は、現在のポリシーを貪欲にすることによって行われる。

この場合、行動価値関数があるので、貪欲policyを構成するためのモデルは必要ない。

任意のaction-value関数$q$に対する貪欲なpolicyは、各$s \in S$に対して、決定論的に行動価値が最大の行動を選択するものである。

$\pi(s) \stackrel{\mathrm{def}}{=} \underset{a} {\argmax}$ $q(s, a)$ $\cdots$ (5.1) 

---
# 5.3 Monte Carlo Control
policy improvementは，各$\pi_{k+1}$を$q_{\pi_{k}}$に関して貪欲なpolicyとして構成することで可能である。

policy improvement theorem (セクション4.2) は$\pi_{k}$と$\pi_{k+1}$に適用される。

なぜなら、すべての$s \in S$に対して、

$$
q_{\pi_{k}}(s, \pi_{k+1}(s)) = q_{\pi_{k}}(s, \underset{a} {\argmax} \space q(s, a)) \\
=\underset{a} {\max} \space q_{\pi_{k}} (s, a) \\
\geqq q_{\pi_{k}} (s, \pi_{k}(s)) \\
\geqq v_{\pi_{k}}(s)
$$

以上から、各$\pi_{k+1}$が$\pi_{k}$よりも一様に優れているか、または$\pi_k$と同じくらい優れていることを保証する。このようにして、サンプルエピソードだけで、最適なpolicyを見つけることができる。

---
# 5.3 Monte Carlo Control


**しかし、このモンテカルロ法の収束保証を簡単に得るために、我々は以下の2つのあり得ない仮定をした。**
- 一つは、エピソードが探索開始の仮定を持つことであり、
- もう一つは、policy評価は無限のエピソード数で行うことができるということである。

実用的なアルゴリズムを得るためには、両方の仮定を取り除く必要がある。

2つめの「policy評価は無限のエピソードで行われる」を取り除くのは比較的簡単である。policy改善に戻る前にpolicy評価を完了することをあきらめることである。

<!-- ---
# 5.3 モンテカルロESの導入
ここで扱われるモンテカルロ法では、エピソードごとに評価と改善を交互に行うのが自然である。

各エピソードの後、観測されたリターンがポリシーの評価に使用され、その後、そのエピソードで訪れたすべての状態においてポリシーが改善される。

このような方法をモンテカルロES(Monte Carlo with Exploring Starts)と呼ぶ。

---
# モンテカルロES
モンテカルロESでは、各状態とアクションのペアのすべてのリターンが、それらが観察されたときにどのようなpolicyが有効であったかに関係なく、累積され、平均化される。モンテカルロESがどのような準最適policyにも収束できないことは容易に理解できる。

もし収束すれば、価値関数は最終的にそのpolicyの価値関数に収束し、その結果、policyが変更されることになる。

安定は、policyと価値関数の両方が最適である場合にのみ達成される。 -->

---
# 5.4 モンテカルロ・コントロール (探索なしスタート)
**1つ目の「探索開始」というありえない仮定を避けるにはどうすればいいのだろうか?** 
- すべての行動が無限に選択されることを保証する唯一の一般的な方法は、エージェントが行動を選択し続けることである。

これを確実にするには2つのアプローチがあり、on policy法とoff policy法と呼ばれるものがある。
- on policy : 意思決定に使用されたポリシーを評価または改善する
- off policy : データを生成するために使用されたポリシーとは異なるポリシーを評価または改善する

---
# 5.4 On policy制御法
本節で紹介するon-policy法は`ε-greedy policy`を用いる
- ほとんどの場合、推定action valueが最大になる行動を選択するが、
- 確率εで、代わりにランダムに行動を選択する。

つまり、すべての非貪欲アクションは最小の選択確率、$ε/A(s)$を与えられ、残りの大部分の確率、$1 - ε + ε/A(s)$が貪欲な行動に与えられる。

しかし、改善点は探索開始の仮定を排除できたことだけ。他は前節と同じ。

これでは、決定論的な方法(e.g. 貪欲)を使えない。

---
# 5.5 Off policy予測の前に・・・
すべての学習制御法はジレンマに直面している:

```
学習制御法は、その後の最適行動を条件として行動値を学習しようとするが、
すべての行動を探索する(最適行動を見つける)ためには非最適行動をとる必要がある。
```

探索的な方針に従って行動しながら、最適な方針を学習するにはどうすればよいのだろうか?

---
# 5.5 Off policy
off policyでは、2つのポリシーを使う。
- 1つは学習され、最適なポリシーとなり、
- もう1つはより探索的で、行動を生成するために使用されます。
  
学習されるポリシーを`target policy`と呼び、行動を生成するために使用されるポリシーを`behavior policy`と呼ぶ。

この場合、学習はターゲットポリシーの「off」データから行われ、全体的なプロセスは`off policy 学習`と呼ばれる。

つまり、オンポリシー法は、targetとbehaviorポリシーが同じ場合の特別なケースであると言える。

---
# 5.5 target, behavior policy
ターゲットと行動ポリシーの両方が固定された予測問題を考える
- すなわち，**我々は $v_{\pi}$ または $q_{\pi}$ を推定したいが，我々が持っているのは別のpolicy $b$ に従ったエピソードだけであるとする。** どちらのポリシーも固定され、与えられたものとみなす。
- bを用いて$\pi$の値を推定するためには、$\pi$のもとで行われたすべてのアクションが、少なくとも時々、bのもとでも行われることが必要である。これをカバレッジの仮定と呼ぶ。
- カバレッジから、$b$は確率的でなければならない。
- 一方、$\pi$は決定論的であってもよい。ここでは、$\pi$が不変であり、与えられたものである予測問題を考える。

---
# 重要度サンプリング比 (importance-sampling ratio)
ほぼすべてのoff policy法は、ある分布の下での期待値を、別の分布からのサンプルで推定する一般的な手法である重要度サンプリングを利用している。

我々は重要度サンプリングをoff policyの学習に適用し、`重要度サンプリング比`と呼ばれるtarget policyとbehavior policyの下で発生する遷移の相対確率に従ってリターンに重み付けを行う。

---
# 重要度サンプリング比 (importance-sampling ratio)
ある開始状態$S_t$が与えられたとき、その後の状態-動作の遷移、$A_t , S_{t+1} , A_{t+1} , . . . , S_T$ は、任意のポリシー$\pi$の下で発生する。

$$
Pr (A_t ,S_{t+1} ,At+1 ,..., S_T |S_t, A_{t:T—1} \thicksim π) \\
= π(A_t|S_t)p(S_{t+1}|S_t , A_t) π(A_{t+1}|S_{t+1}) \cdots p(S_T|S_{T—1} , A_{T—1} ) \\
= \prod_{k=t}^{T -1} \pi(A_{k} |S_{k})p(S_{k+1} |S_{k} , A_{k}) 
$$

ここで$p$は(3.4)で定義される状態遷移確率関数である。

---
# 重要度サンプリング比 (importance-sampling ratio)
従って、target, behaviorの下での遷移の相対確率(重要度サンプリング比)は

$$
\rho_{t:T-1} \stackrel{\mathrm{def}}{=} \frac{\prod_{k=t}^{T -1} \pi(A_{k} |S_{k})p(S_{k+1} |S_{k} , A_{k})}{\prod_{k=t}^{T -1} b(A_{k} |S_{k})p(S_{k+1} |S_{k} , A_{k})} 
= \prod_{k=t}^{T -1} \frac{\pi(A_{k} |S_{k})}{b(A_{k} |S_{k})}
$$

遷移確率は一般に未知であるMDPの遷移確率に依存するが、分子と分母の両方に同じように現れるため、相殺される。

重要度サンプリング比は結局、2つのpolicyとシーケンスにのみ依存し、MDPには依存しない。

---
# 重要度サンプリング比 (importance-sampling ratio)
target policyの下での期待リターン(値)を推定したいが、我々が持っているのはbehavior policyによるリターン$G_t$だけであることを思い出してほしい。

これらのリターンは誤った期待値$E[G_t|S_t=s] = v_b(s)$であるため、平均して$v_{\pi}$を求めることはできない。ここで重要度サンプリングの出番となる。

比率 $\rho_{t:T—1}$ はリターンが正しい期待値を持つように変換する: 
$$
E[\rho_{t:T—1} G_t|S_t=s] = v_{\pi}(s) \cdots (5.4)
$$

これで、policy $b$ に従って観察されたエピソードのバッチからのリターンを平均して、$v_\pi(s)$を推定するモンテカルロ・アルゴリズムを与える準備ができた。

<!-- ---
ここでは，エピソードの境界を越えて増加するように時間ステップを番号付けするのが便利である.

特に、状態 $s$ が訪問されるすべての時間ステップの集合を$\mathscr{T}(s)$と定義することができる。

また、$T(t)$は時刻 $t$ に続く最初の終了時刻を表し、$G_t$ は時刻 $t$ の後、次の時刻まで戻ることを表す。

そして、$({G_t})_{t \in T(s)}$ は状態 $s$ に関係するリターンであり、 $(\rho_{t:T(t)—1})_{t \in T(s)}$は、対応する重要度サンプリング比率である。

$v_{\pi}(s)$を推定するには、単純にリターンを比率でスケーリングし、結果を平均する:

$V(s) \stackrel{\mathrm{def}}{=} \frac{\sum_{t \in \mathscr{T}(s)} \rho_{t:T(t)—1} G_t}{|\mathscr{T}(s)|}$

このように重要度サンプリングが単純平均として行われる場合は、通常の重要度サンプリングと呼ばれる。重要な代替案は、重み付き重要度サンプリングである、と定義される。分母がゼロの場合はゼロとなる。

$V(s) \stackrel{\mathrm{def}}{=} \frac{\sum_{t \in \mathscr{T}(s)} \rho_{t:T(t)—1} G_t}{\sum_{t \in \mathscr{T}(s)} \rho_{t:T(t)—1} }$

---
# 2種類の重要度サンプリング
これらの2種類の重要度サンプリングを理解するために、状態 $s$ からの単一のリターンを観察した後のfirst-visit法の推定値を考えてみましょう。

加重平均推定値では、単一のリターンに対する比率 $\rho_{t:T(t)—1}$ は分子と分母で相殺され、推定値は観測値と等しくなる。比率に依存しないリターン(比率がゼロでないと仮定)。

このリターンが観測された唯一のものであることを考えると，これは妥当な推定値ですが，その期待値は $v_{\pi}(s)$ ではなく $v_b (s)$ であり，この統計的な意味ではバイアスがかかっています。

---
# 2種類の重要度サンプリング
対照的に、通常の重要度サンプリング推定量(5.5)のfirst-visitは，期待値は常に$v_{\pi}(s)$である(不偏である)が，極端になることがある。

この比率が10であったとすると，観察された軌跡は、target policy下ではbehavior policy下の10倍の可能性があることを示す。この場合、通常の重要度サンプリングの推定値は、観察されたリターンの10倍となります。

つまり、そのエピソードの軌跡がターゲット・ポリシーの非常に代表的なものであると考えられるにもかかわらず、観測されたリターンからかなり離れてしまうのです。 -->

---
# 5.6 Incremental Implementation
モンテカルロ予測法は、第2章(セクション2.4)で説明したテクニックを拡張したもので、エピソードごとに段階的に実装することができる。

第2章では報酬を平均化したが、**モンテカルロ法ではリターンを平均化する**。

---
# 通常の重要度サンプリング
通常の重要度サンプリングでは、リターンは重要度サンプリング比 $\rho_{t:T(t)—1}$ でスケーリングされ、(5.5)のように単純に平均化される。状態 $s$ が訪問されるすべての集合を$\mathscr{T}(s)$と定義する。

$$
V(s) \stackrel{\mathrm{def}}{=} \frac{\sum_{t \in \mathscr{T}(s)} \rho_{t:T(t)—1} G_t}{|\mathscr{T}(s)|} \cdots (5,5)
$$

ここでは、リターンの加重平均を形成する必要があり、少し異なるアルゴリズムが必要となる。

---
# 通常の重要度サンプリング
すべてが同じ状態から始まり、それぞれが対応するランダムなウェイト $W_i$ (例えば、$W_i = \rho_{t:T(t)—1}$)を持つリターンのシーケンス$G_1 , G_2 ,... , G_{n—1}$ があるとする。

我々は、次のような推定を行い、それを単一のreturnである$G_n$を得るために、更新し続ける。

$$
V_n \stackrel{\mathrm{def}}{=} \frac{\sum_{k=1}^{n-1} W_k G_t}{\sum_{k=1}^{n-1} W_k}, n \geqq 2
$$

---
$V_n$を追跡することに加えて、各状態に対する最初の$n$個のリターンに与えられた`重みの累積和`$C_n$を保持しなければならない。

$V_n$の更新ルールは
$$
V_{n+1} \stackrel{\mathrm{def}}{=} V_n +\frac{W_n}{C_n}(G_n - V_n), n \geqq 1 \\

and, C_{n+1} \stackrel{\mathrm{def}}{=} C_n + W_{n+1} \\ 

where \space C_{0} = 0, \\

(\space V_1 \space is \space arbitrary \space and \space thus \space need \space not \space be \space specified).
$$

以上から、潜在的に異なるpolicy bに従ってアクションが選択される間、遭遇するすべての状態-アクションのペアに対して、推定return ($q_{\pi}$) に収束する。

---
# 5.7 まとめ
onポリシー法の特徴は、ポリシーの値を推定しながら、それを制御に使うことである。offポリシーでは、この2つの機能は分離させる。

この分離の利点は、
- ターゲット・ポリシーが決定論的(例えば、貪欲)である可能性がある一方で、
- ビヘイビア・ポリシーがすべての可能なアクションをサンプリングし続けることができることである。

つまり、ターゲット・ポリシーを学習し改善しながら、行動ポリシーに従う。これらの技法は、target policyによって選択される可能性のある全ての行動の選択確率が0でないこと(カバレッジ)を必要とする。

全ての可能性を探索するために、行動方針がソフトであること(すなわち、全ての状態において全ての行動をゼロでない確率で選択すること)を要求する。

---
# 5.7 Offポリシー・モンテカルロ制御の問題点
潜在的な問題は、この方法がエピソードの最後尾からしか学習しないということである。

もし貪欲でない行動が一般的であれば、特に長いエピソードの初期に現れる状態の学習は遅くなる。これは学習を大幅に遅らせる可能性がある。

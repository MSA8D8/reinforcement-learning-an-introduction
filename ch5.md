---
marp: true
theme: default
math : MathJax


---
<!-- paginate: true -->
# Chapter 5: Monte Carlo Methods
##### 2023.12.1
##### 佐々木

<!-- <style scoped>
section { font-size: 23px; }
</style> -->

---
# Monte Carlo methodsの導入
- 本章では、価値関数を推定し、最適な方針を発見するための最初の学習法を考える。**前章とは異なり、ここでは環境の完全な知識を前提としない。**
- モンテカルロ法は、実際の、あるいはシミュレートされた環境から得られた、状態、行動、報酬のサンプルシーケンスである経験だけを必要とする。
- モデルは必要であるが、 **動的計画法(DP)に必要なすべての可能な遷移の完全な確率分布ではなく、サンプルの遷移を生成するだけでよい。**

---
# Monte Carlo methods 
モンテカルロという用語 : 操作に重要なランダム要素が含まれるあらゆる推定方法に対して、より広義に使用されることが多い。

ここでは特に、**完全なリターン(= complete return)の平均に基づく方法**(部分的なリターンから学習する方法は次の章で検討)に使用する。

- well-definedされたリターンが得られるように、**ここではモンテカルロ法をエピソード・タスクに対してのみ定義する。**
  - つまり、**経験はエピソードに分割され、どのような行動を選択しても、すべてのエピソードは最終的に終了すると仮定する。**
  - エピソードが終了したときのみ、valueのestimateとactionが変更される。
  - したがって、エピソードごとの意味ではインクリメンタルであるが、ステップバイステップ(オンライン)の意味ではインクリメンタルではない。

---
# Monte Carlo methods 
モンテカルロ法は、
- 各状態とアクションのペアのリターンをサンプリングして平均する。
- つまり、ある状態で行動をとった後のリターンは、同じエピソード内の後の状態でとった行動に依存する。
- よって、すべての行動選択は学習中であるため、先の状態から見て問題は非定常となる。

---
# Monte Carlo for episodic tasks
非定常性を扱うために、第4章で紹介した**GPI(general policy iteration)の考え方をDPに適用する。** 
  - 4章では、MDPの知識から価値関数を計算したが、ここではMDPを用いたサンプルリターンから価値関数を学習する。
  - 価値関数とそれに対応するpolicyは、基本的に同じ方法(GPI)で最適性を達成するために相互作用する。

DPの章と同様に、
- まず予測問題を考える (the computation of $v_{\pi}$ and $q_{\pi}$ for a fixed arbitrary policy $\pi$)
- 次に、policy imporovement
- 最後に、GPIによる制御問題とその解法。

DPからの各アイデアは、サンプル経験のみが利用可能なモンテカルロ法に拡張される

---
# 5.1 Monte Carlo Prediction
**まず、与えられたpolicyの状態-価値関数を学習するモンテカルロ法を考える。** 

- ある状態($s$)の価値($v$)とは、その状態からの期待リターン(将来の割引報酬の累積期待値)であることを思い出してください。

- 経験則からこの値を推定する明白な方法は、その状態を訪れた後に観察されたリターンを単純に平均すること。

- より多くのリターンが観測されれば、平均値は期待値に収束するはずである。

この考え方は、すべてのモンテカルロ法の根底にある。

---
# 5.1 Monte Carlo Prediction
**あるポリシー$\pi$に従って$s$を通過することによって得られるエピソードの集合が与えられたとき、ポリシーの下での状態$s$の値$v_{\pi}(s)$を推定したいとする。**

あるエピソードにおける状態$s$の各出現は、$s$への訪問(visit)と呼ばれる。もちろん、$s$は同じエピソードで複数回訪問されるかもしれない。あるエピソードで$s$が最初に訪問されたときを、$s$へのfirst-visitと呼ぶことにする。

- first-visit MCの場合、$v_{\pi}(s)$を$s$への初回訪問後のリターンの平均として推定する
- 一方、毎回訪問する(every-visit)場合は、$s$へのすべての訪問後のリターンを平均する

これら2つのモンテカルロ法は非常によく似ているが、理論的性質は若干異なる。**本章で取り上げるのもFirst-visit MC法である。**

---
<style scoped>
section { font-size: 20px; }
</style>

# First-visit MCのアルゴリズム : estimate $V \approx v_{\pi}$

Input: 
- 評価されるpolicy $\pi$ 

初期化:
- $V (s) \in R$、任意、すべての $s \in S$ に対して
- $Returns(s)$ ← すべての $s \in S$ に対して、空のリスト。

永遠にループする(各エピソードごとに):
- $\pi$に続くエピソードを生成する:$s_{0} , a_{0} , r_{1} , s_{1} , a_{1} , r_{2} ,..., s_{T—1} , a_{T—1} , r_{T}$ 
- $g ← 0$ 
- エピソードの各ステップ、t = T -1,T -2, ..., 0に対してループ: 
  - $G ← \gamma G + R_{t+1}$ ($\gamma$は割引率)
  - $S_{t}$が$S_{0}, S_{1} ,..., S_{t—1}$に出現しない限り:
    - $Returns(S_{t})$に$G$を追加する。
    - $V(S_{t}) ← average(Returns(S_{t}))$


---
# Example 5.1: Blackjack 
- 二枚のカードの合計が21に近い方が勝ち。21を超えると負け。21以下ならカードを引くか引かないか (hit or stick) を選択できる。
- 絵札は10, aceは1 or 11になる.
- **The game begins with two cards dealt to both dealer and player.**
  - **One of the dealer’s cards is face up and the other is face down.**
- ディーラーは、17以上になるまでカードを引く。17以上になったら、引かない。21を超えたら負け。21以下なら、プレイヤーと比較して勝ち負けが決まる。

---
# Example 5.1: Blackjack 
ブラックジャックをプレイすることは，当然ながらエピソード有限MDPとして定式化される. 
- Each game of blackjack is an episode. 
- Rewards of +1, -1, and 0 are given for winning, losing, and drawing, respectively. 
- All rewards within a game are zero, and we do not discount ($\gamma$ = 1); 
- therefore these terminal rewards are also the returns. 
- The player’s actions are to hit or to stick. The states depend on the player’s cards and the dealer’s showing card. 


--- 
# Example 5.1: Blackjack 
**We assume that cards are dealt from an infinite deck (i.e., with replacement) so that there is no advantage to keeping track of the cards already dealt.** ← 重要な前提条件(現実とは違う)。

もし、playerがaceを11として使えるなら、それをusableと定義する。usableなら、hitしたほうが絶対に良い(ace = 1 or 11)。

Thus, the player makes decisions on the basis of three variables: 
 - his current sum(12–21), 
 - the dealer’s one showing card (ace–10), 
 - and whether or not he holds a usable ace. 

This makes for a total of 200 states.

---
# Example 5.1: Blackjack 
プレイヤーが20か21であればstandし，そうでなければhitするというpolicyを考える.

**このポリシーのstate-value関数をモンテカルロ法で求めるには、このpolicyを使ってブラックジャックゲームをたくさんシミュレートし，各状態に続くリターンを平均化する.**

このようにして，図5.1に示す状態値関数の推定値が得られた (教科書参照).

---
# Example 5.1: Blackjack 
**ブラックジャックのタスクでは環境に関する完全な知識を持っているが、DP法を適用して価値関数を計算することは容易ではない。**
- 例えば，プレイヤーが14で，スティック=standを選んだとする。ディーラーがカードを出したときの関数として，プレイヤーの報酬が+1される確率は?

- DPを適用する前に、すべての確率を計算しなければならないが、このような計算は複雑でエラーが発生しやすいことが多い。対照的に、モンテカルロ法で必要なサンプルゲームの生成は簡単である。

- モンテカルロ法の重要な点は、各状態の推定値が独立していることである。DPの場合のように、ある状態の推定値が他の状態の推定値の上に構築されることはない。

---
# 5.2 Estimation of Action Values (modelがない場合)
モデルがあれば、状態値だけでpolicyを決めるのに十分である。DPの章でやったように、一歩先を見て、報酬と次の状態の最良の組み合わせにつながる行動を選ぶだけである。

**しかし、モデルがなければ、状態値だけでは十分ではない。** 
  - その値をpolicy提案に役立てるために、各行動の値を明示的に推定する必要がある
  - したがって、モンテカルロ法の主要な目標の1つは、$q_{*}$を推定することである。
  - これを達成するために、**まず、action-valueに対するpolicyの評価問題を考える。**

---
# action-valueに対するpolicy評価の問題
行動に対するpolicy評価の問題 : 
- 状態 $s$ から始めて行動 $a$ をとり、その後policy $\pi$ に従うときの期待リターン$q_{\pi}(s, a)$を推定すること。
- ただし、ここでは状態への訪問ではなく、状態とアクションのペアへの訪問について述べる。
- 状態-行動ペア$(s,a)$は、あるエピソードにおいて、状態$s$が訪問され、その中で行動$a$が取られた場合に、訪問されたという。

唯一の複雑な点は、**多くの状態とアクションのペアが一度も訪問されない可能性があることである。**

---
# 行動に対するpolicy評価の問題
もし平均するリターンがない場合、モンテカルロ法による他のアクションは経験によって向上することはない。これは深刻な問題である。選択肢を比較するためには、各状態のすべての行動の価値を推定する必要がある。

そこで、エピソードが無限に続く限り、すべての状態アクション・ペアが無限に訪れることを保証する。これを**探索開始の仮定**と呼ぶ(この仮定を取り除く方法が後に導入される。現実的にはありえない仮定であるから)。

今のところ、探索開始の仮定を維持し、完全なモンテカルロ制御法の提示を完了する。

---
# 5.3 Monte Carlo Control
これで、モンテカルロ推定から最適なポリシーを近似することについて考える準備ができた。

全体的な考え方は、DPの章と同じパターンの考え方に従って進めることである。

GPIでは、近似的な方針と近似的な価値関数の両方を維持する。
- 価値関数は現在の方針の価値関数により近くなるように繰り返し変更され、
- 方針は現在の価値関数に対して繰り返し改善される。

この2種類の変更を一緒に行うことで、policyと価値関数の両方が最適に近づく。

---
# 5.3 Monte Carlo Control
はじめに、古典的なpolicy反復を考えてみよう。この方法では、policy評価とpolicy反復の完全なステップを交互に実行する。任意のpolicy$\pi_{0}$から始まり、最適なpolicyと最適な行動価値関数で終わる:

$$
\pi_{0}\stackrel{E}{\longrightarrow}q_{\pi_{0}}\stackrel{I}{\longrightarrow}\pi_{1}\stackrel{E}{\longrightarrow}q_{\pi_{1}}\stackrel{I}{\longrightarrow}\pi_{2}\cdots
$$

ここで、
- $\stackrel{E}{\longrightarrow}$は完全なpolicy(complete policy)評価を表し、
- $\stackrel{I}{\longrightarrow}$は完全なpolicy improvementを表す。

とりあえず、本当に無限のエピソードを観察し、さらに、エピソードが次のように生成されると仮定しよう。これらの仮定の下で、モンテカルロ法は、任意の$q_{\pi_{k}}$に対して、各$\pi_{k}$を正確に計算する。

---
# 5.3 Monte Carlo Control
ポリシーの改善は、現在のポリシーを貪欲にすることによって行われる。

この場合、行動価値関数があるので、貪欲policyを構成するためのモデルは必要ない。

任意のaction-value関数$q$に対する貪欲なpolicyは、各$s \in S$に対して、決定論的に行動価値が最大の行動を選択するものである。

$\pi(s) \stackrel{\mathrm{def}}{=} \underset{a} {\argmax}$ $q(s, a)$ $\cdots$ (5.1) 

---
# 5.3 Monte Carlo Control
policy improvementは，各$\pi_{k+1}$を$q_{\pi_{k}}$に関して貪欲なpolicyとして構成することで可能である。

policy improvement theorem(セクション4.2)は$\pi_{k}$と$\pi_{k+1}$に適用される。

なぜなら、すべての$s \in S$に対して、

$$q_{\pi_{k}}(s, \pi_{k+1}(s)) = q_{\pi_{k}}(s, \underset{a} {\argmax} q(s, a))$$
$$=\underset{a} {\max} q_{\pi_{k}} (s, a)$$
$$\geqq q_{\pi_{k}} (s, \pi_{k}(s))$$ 
$$\geqq v_{\pi_{k}}(s)$$

---
# 5.3 Monte Carlo Control
以上から、各$\pi_{k+1}$が$\pi_{k}$よりも一様に優れているか、または$\pi_k$と同じくらい優れていることを保証する。このようにして、サンプルエピソードだけで、最適なpolicyを見つけることができる。

**しかし、このモンテカルロ法の収束保証を簡単に得るために、我々は以下の2つのあり得ない仮定をした。**
- 一つは、エピソードが探索開始を持つことであり、
- もう一つは、policy評価は無限のエピソード数で行うことができるということである。

実用的なアルゴリズムを得るためには、両方の仮定を取り除く必要がある。

---
# 5.3 Monte Carlo Control (無限のエピソード数)
2つめの「policy評価は無限のエピソードで行われる」という、ありえない仮定に注目する。

この仮定を取り除くのは比較的簡単である。問題を解決する方法は2つあります。
1. 各policy評価において$q_{\pi_{k}}$を近似するという考えを堅持することである。
2. policy改善に戻る前にpolicy評価を完了することをあきらめることである。

---
# 1. 各policy評価において$q_{\pi_{k}}$を近似するという考えを堅持することである。
推定値の誤差の大きさと確率の境界を得るために測定と仮定を行い、各policy評価の間に十分なステップを踏んで、これらの境界が十分に小さいことを保証します。

このアプローチは、ある程度の近似レベルまでは正しい収束を保証するという意味で、おそらく完全に満足のいくものにできるだろう。

しかし、最小の問題以外では、実際には有用でないほど多くのエピソードを必要とする可能性が高い。

---
# 2. policy改善に戻る前にpolicy評価を完了することをあきらめることである。
各評価ステップで価値関数を$q_{\pi_{k}}$に近づけますが、多くのステップを経なければ実際に近づくことは期待できません。

この考え方の一つの極端な形態は、値の反復であり、この場合、ポリシーの改善の各ステップの間に、ポリシーの反復評価の1つの反復のみが実行される。インプレース版の値反復はさらに極端で、1つの状態に対して改善と評価のステップを交互に行います。

---
# 5.3 Monte Carlo Control (無限のエピソード数)
モンテカルロ・ポリシーの反復では、エピソードごとに評価と改善を交互に行うのが自然である。各エピソードの後、観測されたリターンがポリシーの評価に使用され、その後、そのエピソードで訪れたすべての状態においてポリシーが改善される。

モンテカルロES(Monte Carlo with Exploring Starts)と呼ぶ、このような単純なアルゴリズムが、次ページのボックスに擬似コードで示されています。

---
# モンテカルロES
モンテカルロESでは、各状態とアクションのペアのすべてのリターンが、それらが観察されたときにどのようなpolicyが有効であったかに関係なく、累積され、平均化される。モンテカルロESがどのような準最適policyにも収束できないことは容易に理解できる。

もし収束すれば、価値関数は最終的にそのpolicyの価値関数に収束し、その結果、policyが変更されることになる。

安定は、policyと価値関数の両方が最適である場合にのみ達成される。

---
<style scoped>
section { font-size: 20px; }
</style>

# モンテカルロESのアルゴリズム : estimate $\pi \approx \pi_{*}$
初期化:
- $V (s) \in R$、任意、すべての $s \in S$ に対して
- $Returns(s)$ ← すべての $s \in S$ に対して、空のリスト。

永遠にループする(各エピソードごとに):
- $\pi$に続くエピソードを生成する:$s_{0} , a_{0} , r_{1} , s_{1} , a_{1} , r_{2} ,..., s_{T—1} , a_{T—1} , r_{T}$ 
- $g ← 0$ 
- エピソードの各ステップ、t = T -1,T -2, ..., 0に対してループ: 
  - $G ← \gamma G + R_{t+1}$ ($\gamma$は割引率)
  - $S_{t}$が$S_{0}, S_{1} ,..., S_{t—1}$に出現しない限り:
    - $Returns(S_{t})$に$G$を追加する。
    - $V(S_{t}) ← average(Returns(S_{t}))$

---
# 5.4 モンテカルロ・コントロール (探索なしスタート)
**1つ目の「探索開始」というありえない仮定を避けるにはどうすればいいのだろうか?** 
- すべての行動が無限に選択されることを保証する唯一の一般的な方法は、エージェントが行動を選択し続けることである。

これを確実にするには2つのアプローチがあり、on policy法とoff policy法と呼ばれるものがある。
- on policy : 意思決定に使用されたポリシーを評価または改善する
- off policy : データを生成するために使用されたポリシーとは異なるポリシーを評価または改善する

モンテカルロES法はオンポリシー法の一例である。

本節では、探索開始という非現実的な仮定を用いない `on policy モンテカルロ制御法` を設計する方法を示す。

---
# 5.4 On policy制御法
On policy制御法では、
- ポリシーは一般的にソフト 
  - $\forall s \in S, \forall a \in A (s)$に対して$\pi(a, s) > 0$を意味する
- だが、徐々に決定論的なポリシーに近づいていく。

本節で紹介するon-policy法は`ε-greedy policy`を用いる
- ほとんどの場合、推定action valueが最大になる行動を選択するが、
- 確率εで、代わりにランダムに行動を選択する。

つまり、すべての非貪欲アクションは最小の選択確率、$ε/A(s)$を与えられ、残りの大部分の確率、$1 - ε + ε/A(s)$が貪欲な行動に与えられる。このとき、$\pi(a|s) \geqq ε/A(s)$ (定義) となる。

---
# 5.4 モンテカルロ・コントロール (探索なしスタート)
ここで提案されたOn policy制御の全体的な考え方は、やはりGPIである。

モンテカルロESと同様に、first-visit MC法を用いて、現在のポリシーの行動価値関数を推定する。

しかし、**探索開始の仮定がなければ、現在の価値関数に関して貪欲にすることで単純にポリシーを改善することはできません。**

```
※ 5.4　最後の数ページの数式については、教科書を参照のこと。
- 要するに、ポリシーの反復が有効であることを示している。
- 改善点は探索開始の仮定を排除できたことだけ。他は前節と同じ。
```

---
# 5.5 重要度サンプリングによるOffpolicy予測
すべての学習制御法はジレンマに直面している:

```
学習制御法は、その後の最適行動を条件として行動値を学習しようとするが、
すべての行動を探索する(最適行動を見つける)ためには非最適行動をとる必要がある。
```

探索的な方針に従って行動しながら、最適な方針を学習するにはどうすればよいのだろうか?

---
# 5.5 重要度サンプリングによるOffpolicy予測
前節のOn policyアプローチは実際には妥協の産物であり、最適なポリシーではなく、探索を続ける最適に近いポリシーについてaction, valueを学習する。

もっと簡単な方法は、2つのポリシーを使うことだ。
- 1つは学習され、最適なポリシーとなり、
- もう1つはより探索的で、行動を生成するために使用されます。
  
学習されるポリシーを`target policy`と呼び、行動を生成するために使用されるポリシーを`behavior policy`と呼ぶ。

この場合、学習はターゲットポリシーの「off」データから行われ、全体的なプロセスは`off policy 学習`と呼ばれる。

---
# 5.5 重要度サンプリングによるOffpolicy予測
**本書の残りの部分では、onポリシーとoffポリシーの両方を検討する。**
- on policy法は一般的に単純であり、最初に考察する。
- off policy法は、追加の概念と表記法を必要とし、データが異なるポリシーによるものであるため、Offポリシー法は多くの場合分散が大きく、収束が遅い。その一方で、より強力で一般的である。

つまり、オンポリシー法は、targetとbehaviorポリシーが同じ場合の特別なケースであると言える。

---
# 5.5 target, behavior policy
このセクションでは、ターゲットと行動ポリシーの両方が固定された予測問題を考えることによって、offポリシー法の研究を開始する。
- すなわち，**我々は $v_{\pi}$ または $q_{\pi}$ を推定したいが，我々が持っているのは別のpolicy $b$ に従ったエピソードだけであるとする。** どちらのポリシーも固定され、与えられたものとみなす。
- bのエピソードを用いて$\pi$の値を推定するためには、$\pi$のもとで行われたすべてのアクションが、少なくとも時々、bのもとでも行われることが必要である。
- つまり、$π(a|s) > 0, b(a|s) > 0$を意味する。これをカバレッジの仮定と呼ぶ。
- カバレッジから、$b$は$\pi$と同一でない状態では確率的でなければならないことがわかる。
- 一方、$\pi$は決定論的であってもよい。この節では、$\pi$が不変であり、与えられたものである予測問題を考える。
  
---
# 重要度サンプリング比 (importance-sampling ratio)
ほとんどすべてのoff policy法は、ある分布の下での期待値を別の分布からのサンプルで推定する一般的な手法である重要度サンプリングを利用している。我々は重要度サンプリングをoff学習に適用し、`重要度サンプリング比`と呼ばれるtarget policyとbehavior policyの下で発生する遷移の相対確率に従ってリターンに重み付けを行う。

---
# 重要度サンプリング比 (importance-sampling ratio)
ある開始状態$S_t$が与えられたとき、その後の状態-動作の遷移、$A_t , S_{t+1} , A_{t+1} , . . . , S_T$ は、任意のポリシー$\pi$の下で発生する。

$$
Pr (A_t ,S_{t+1} ,At+1 ,..., S_T |S_t, A_{t:T—1} \thicksim π) \\
= π(A_t|S_t)p(S_{t+1}|S_t , A_t) π(A_{t+1}|S_{t+1}) \cdots p(S_T|S_{T—1} , A_{T—1} ) \\
= \prod_{k=t}^{T -1} \pi(A_{k} |S_{k})p(S_{k+1} |S_{k} , A_{k}) 
$$

ここで$p$は(3.4)で定義される状態遷移確率関数である。

---
# 重要度サンプリング比 (importance-sampling ratio)
従って、target, behaviorの下での遷移の相対確率(重要度サンプリング比)は

$$
\rho_{t:T-1} \stackrel{\mathrm{def}}{=} \frac{\prod_{k=t}^{T -1} \pi(A_{k} |S_{k})p(S_{k+1} |S_{k} , A_{k})}{\prod_{k=t}^{T -1} b(A_{k} |S_{k})p(S_{k+1} |S_{k} , A_{k})} 
= \prod_{k=t}^{T -1} \frac{\pi(A_{k} |S_{k})}{b(A_{k} |S_{k})}
$$

遷移確率は一般に未知であるMDPの遷移確率に依存するが、分子と分母の両方に同じように現れるため、相殺される。

重要度サンプリング比は結局、2つのpolicyとシーケンスにのみ依存し、MDPには依存しない。

---
# 重要度サンプリング比 (importance-sampling ratio)
target policyの下での期待リターン(値)を推定したいが、我々が持っているのはbehavior policyによるリターン$G_t$だけであることを思い出してほしい。

これらのリターンは誤った期待値$E[G_t|S_t=s] = v_b(s)$であるため、平均して$v_{\pi}$を求めることはできない。ここで重要度サンプリングの出番となる。

比率 $\rho_{t:T—1}$ はリターンが正しい期待値を持つように変換する: 
$$
E[\rho_{t:T—1} G_t|S_t=s] = v_{\pi}(s) \cdots (5.4)
$$

これで、policy $b$ に従って観察されたエピソードのバッチからのリターンを平均して、$v_\pi(s)$を推定するモンテカルロ・アルゴリズムを与える準備ができた。

---
ここでは，エピソードの境界を越えて増加するように時間ステップを番号付けするのが便利である.

すなわち，バッチの最初のエピソードが時間100の終端状態で終了する場合，次のエピソードは時間t = 101で始まる.

これにより、特定のエピソードの特定のステップを参照するために、タイム・ステップ番号を使用することができる。

特に、状態 $s$ が訪問されるすべての時間ステップの集合を$\mathscr{T}(s)$と定義することができる。

これはevery-visitメソッドの場合であり、first-visitメソッドの場合、$\mathscr{T}(s)$はエピソード内で $s$ への最初の訪問である時間ステップのみを含む。

---
また、$T(t)$は時刻 $t$ に続く最初の終了時刻を表し、$G_t$ は時刻 $t$ の後、次の時刻まで戻ることを表す。

そして、$({G_t})_{t \in T(s)}$ は状態 $s$ に関係するリターンであり、 $(\rho_{t:T(t)—1})_{t \in T(s)}$は、対応する重要度サンプリング比率である。

$v_{\pi}(s)$を推定するには、単純にリターンを比率でスケーリングし、結果を平均する:

$V(s) \stackrel{\mathrm{def}}{=} \frac{\sum_{t \in \mathscr{T}(s)} \rho_{t:T(t)—1} G_t}{|\mathscr{T}(s)|}$

このように重要度サンプリングが単純平均として行われる場合は、通常の重要度サンプリングと呼ばれる。重要な代替案は、重み付き重要度サンプリングである、と定義される。分母がゼロの場合はゼロとなる。

$V(s) \stackrel{\mathrm{def}}{=} \frac{\sum_{t \in \mathscr{T}(s)} \rho_{t:T(t)—1} G_t}{\sum_{t \in \mathscr{T}(s)} \rho_{t:T(t)—1} }$

---
# 2種類の重要度サンプリング
これらの2種類の重要度サンプリングを理解するために、状態 $s$ からの単一のリターンを観察した後のfirst-visit法の推定値を考えてみましょう。

加重平均推定値では、単一のリターンに対する比率 $\rho_{t:T(t)—1}$ は分子と分母で相殺され、推定値は観測値と等しくなる。比率に依存しないリターン(比率がゼロでないと仮定)。

このリターンが観測された唯一のものであることを考えると，これは妥当な推定値ですが，その期待値は $v_{\pi}(s)$ ではなく $v_b (s)$ であり，この統計的な意味ではバイアスがかかっています。

---
# 2種類の重要度サンプリング
対照的に、通常の重要度サンプリング推定量(5.5)のfirst-visitは，期待値は常に$v_{\pi}(s)$である(不偏である)が，極端になることがある。

この比率が10であったとすると，観察された軌跡は、target policy下ではbehavior policy下の10倍の可能性があることを示す。この場合、通常の重要度サンプリングの推定値は、観察されたリターンの10倍となります。

つまり、そのエピソードの軌跡がターゲット・ポリシーの非常に代表的なものであると考えられるにもかかわらず、観測されたリターンからかなり離れてしまうのです。

---
形式的には，2種類の重要度サンプリングの初回訪問法の間の差は，それらの偏りと分散で表現される.

通常の重要度サンプリングは不偏であるのに対して、重み付き重要度サンプリングは偏りがある(ただし、偏りは漸近的にゼロに収束する)。一方，通常の重要度サンプリングの分散は，一般に，比率の分散が束縛されないので，束縛されませんが，重み付き推定では，単一のリター ンの最大の重みは1です.

---
実際、リターンが有界であると仮定すると、比率自体 の分散が無限大であっても、重み付き重要度サンプリング推定量の分散はゼロに収束します(Precup, Sutton, and Dasgupta 2001)。実際には，重み付き推定量は通 常，分散が劇的に小さく，強く好まれる。とはいえ，通常の重要度サンプリングは，本書の第2部で検討する関数近似を用いた近似手法に拡張する方が簡単であるため，完全に放棄するつもりはない. 

通常の重要度サンプリングと重み付き重要度サンプリングのEvery-visit法は、どちらも偏りがあるが、これもサンプル数が増えるにつれて偏りは漸近的に ゼロになる。実際には、どの状態が訪問されたかを追跡する必要がなく、ま た近似への拡張がより簡単であるため、every-visit法が好まれることが多い。重み付き重要度サンプリングを用いたo↵ポリシーpolicy評価のための完全な every-visit MCアルゴリズムは、110ページの次のセクションで与えられる。


---
# 5.6 Incremental Implementation
モンテカルロ予測法は、第2章(セクション2.4)で説明したテクニックを拡張し たもので、エピソードごとに段階的に実装することができる。第 2 章では報酬 を平均化したが、モンテカルロ法ではリターンを平均化する。その他の点では 、第2章で使用したのとまったく同じ方法が、オン・ポリシー・モンテ・カルロ 法にも使用できる。o↵ポリシー・モンテカルロ法については、通常の重要度サ ンプリングを使用するものと、重み付き重要度サンプリングを使用するものを 分けて考える必要がある。

---
通常の重要度サンプリングでは、リターンは重要度サンプリング比ρ t:T(t)— 1 (5.3)でスケーリングされ、(5.5)のように単純に平均化される。これらの手 法では、第2章の漸増法を再び使うことができるが、スケーリングされたリ ターンの代わりに
はその章の報酬である。これは、重み付き重要度サンプリングを使用したo↵ポ リシー法の場合を残す。ここでは、リターンの加重平均を形成する必要があり 、少し異なるインクリメンタルアルゴリズムが必要となる。
G1 , G2 ,... , Gn—1 , すべてが同じ状態から始まり、それぞれが対応するラ ンダムなウェイト Wi (例えば、Wi = ρt i :T (ti )-1)を持つリターンのシーケン スがあるとする。我々は、次のような推定を行いたい。

---
nVn を追跡することに加えて、各状態について、最初のn個のリターンに与えら
—週 k=1
れた重みの累積和Cn を保持しなければならない。Vn の更新ルールは
n ≥ 1、
Vn+1 = V n そして
G n - Vn ,
(5.8)
. Wnh i
Cn+1 . =Cn + Wn+1、
Cn
.

---
ここで、C0 = 0(およびV1 は任意であるため、指定する必要はない)。上の ボックスは
次のページには、モンテカルロ・ポリシー評価のための完全なエピソードごと のインクリメンタル・アルゴリズムが含まれている。このアルゴリズムは，名

目上は，重み付き重要度サンプリングを用いたo↵ポリシーの場合のためのもの
ですが，次のように選択するだけで，on-ポリシーの場合にも適用できます. ターゲット・ポリシーとビヘイビア・ポリシーを同じにする(この場合(π=
b)、Wは常に1)。このとき Q の近似値は q⇡ に収束する。
は、潜在的に異なるポリシーbに従って選択される。


---
# 5.7 Offポリシー・モンテカルロ制御
ここで、本書で考察する2番目のクラスの学習制御法の例を紹介しよう。オンȕ ポリシー法の特徴は、ポリシーの値を推定しながら、それを制御に使うことで ある。oȕポリシー・メソッドでは、この2つの機能は分離されています。振る舞 いポリシーと呼ばれる、振る舞いを生成するために使用されるポリシーは、タ ーゲットポリシーと呼ばれる、評価され改善されるポリシーとは無関係である 可能性があります。この分離の利点は、ターゲット・ポリシーが決定論的(例 えば、貪欲)である可能性がある一方で、ビヘイビア・ポリシーがすべての可 能なアクションをサンプリングし続けることができることである。

---
O↵policyモンテカルロ制御法は、前の2つのセクションで紹介した手法のい ずれかを使用する。モンテカルロ制御法は、ターゲット・ポリシーを学習し 改善しながら、行動ポリシーに従う。これらの技法は、行動方針が、目標方 針によって選択される可能性のある全ての行動を選択する確率が0でないこ と(カバレッジ)を必要とする。全ての可能性を探索するために、行動方針 がソフトであること(すなわち、全ての状態において全ての行動をゼロでな い確率で選択すること)を要求する。

---
次ページのボックスは、GPI と重み付き重要度サンプリングに基づく、π* とq* を推定するための oポリシー・モンテカルロ制御法を示しています。 目標 ≈ポリシー π π* は、q⇡ の推定値であるQ に関する貪欲なポリシーです。

policybはどのようなものでも良いが、πが最適policyに収束することを保証するためには、πが最 適policyに収束するようなpolicyが必要である。各状態とアクションのペアに対して、無限のリターンを得なければならない。 これはbをε-softに選ぶことで保証できる。policyπは、行動ȕが異なるソフトpolicyb にしたがって選択されても、遭遇するすべての状態において最適に収束する。

エピソード間、あるいはエピソード内でも変化する。潜在的な問題は、この方法がエピソードの最後尾からしか学習しないということである。もし貪欲でない行動が一般的であれば、特に長いエピソードの初期に現れる状態の学習は遅くなる。これは学習を大幅に遅らせる可能性がある。

---
この問題がどの程度深刻かを評価するには、オフポリシー・モンテカルロ法
の経験が不十分である。この問題が深刻であるとすれば、最も重要な対処法は
、おそらく時間的モンテカルロ法を取り入れることであろう。
差分学習は、次章で開発されるアルゴリズムのアイデアである。あるいは、ц が1より小さい場合、次のセクションで開発されたアイデアも大いに役立つだろ う。

---
marp: true
theme: default
math : katex


---
<!-- paginate: true -->
# Chapter 5: Monte Carlo Methods
##### 2023.12.1
##### 佐々木

<!-- <style scoped>
section { font-size: 23px; }
</style> -->

---
# In this chapter = Monte Carlo methodsの導入
- 本章では、価値関数を推定し、最適な方針を発見するための最初の学習法を考える。**前章とは異なり、ここでは環境の完全な知識を前提としない。**
- モンテカルロ法は、実際の、あるいはシミュレートされた環境から得られた、状態、行動、報酬のサンプルシーケンスである経験だけを必要とする。
- モデルは必要であるが、 **動的計画法(DP)に必要なすべての可能な遷移の完全な確率分布ではなく、サンプルの遷移を生成するだけでよい。**

---
# Monte Carlo methods 
モンテカルロという用語 : 操作に重要なランダム要素が含まれるあらゆる推定方法に対して、より広義に使用されることが多い。

ここでは特に、**完全なリターン(= complete return)の平均に基づく方法**(部分的なリターンから学習する方法は次の章で検討)に使用する。

- well-definedされたリターンが得られるように、**ここではモンテカルロ法をエピソード・タスクに対してのみ定義する。**
  - つまり、**経験はエピソードに分割され、どのような行動を選択しても、すべてのエピソードは最終的に終了すると仮定する。**
  - エピソードが終了したときのみ、valueのestimateとactionが変更される。
  - したがって、エピソードごとの意味ではインクリメンタルであるが、ステップバイステップ(オンライン)の意味ではインクリメンタルではない。

---
# Monte Carlo methods 
モンテカルロ法は、
- 各状態とアクションのペアのリターンをサンプリングして平均する。
- つまり、ある状態で行動をとった後のリターンは、同じエピソード内の後の状態でとった行動に依存する。
- よって、すべての行動選択は学習中であるため、先の状態から見て問題は非定常となる。

---
# Monte Carlo for episodic tasks
非定常性を扱うために、第4章で紹介した**GPI(general policy iteration)の考え方をDPに適用する。** 
  - 4章では、MDPの知識から価値関数を計算したが、ここではMDPを用いたサンプルリターンから価値関数を学習する。
  - 価値関数とそれに対応するpolicyは、基本的に同じ方法(GPI)で最適性を達成するために相互作用する。

DPの章と同様に、
- まず予測問題を考える (the computation of $v_{\pi}$ and $q_{\pi}$ for a fixed arbitrary policy $\pi$)
- 次に、policy imporovement
- 最後に、GPIによる制御問題とその解法。

DPからの各アイデアは、サンプル経験のみが利用可能なモンテカルロ法に拡張される

---
# 5.1 Monte Carlo Prediction
**まず、与えられたpolicyの状態-価値関数を学習するモンテカルロ法を考える。** 

- ある状態($s$)の価値($v$)とは、その状態からの期待リターン(将来の割引報酬の累積期待値)であることを思い出してください。

- 経験則からこの値を推定する明白な方法は、その状態を訪れた後に観察されたリターンを単純に平均すること。

- より多くのリターンが観測されれば、平均値は期待値に収束するはずである。

この考え方は、すべてのモンテカルロ法の根底にある。

---
# 5.1 Monte Carlo Prediction
**あるポリシー$\pi$に従って$s$を通過することによって得られるエピソードの集合が与えられたとき、ポリシーの下での状態$s$の値$v_{\pi}(s)$を推定したいとする。**

あるエピソードにおける状態$s$の各出現は、$s$への訪問(visit)と呼ばれる。もちろん、$s$は同じエピソードで複数回訪問されるかもしれない。あるエピソードで$s$が最初に訪問されたときを、$s$へのfirst-visitと呼ぶことにする。

- first-visit MCの場合、$v_{\pi}(s)$を$s$への初回訪問後のリターンの平均として推定する
- 一方、毎回訪問する(every-visit)場合は、$s$へのすべての訪問後のリターンを平均する

これら2つのモンテカルロ法は非常によく似ているが、理論的性質は若干異なる

---
# Two MC methods
First-visit MC : 1940年代までさかのぼり、最も広く研究されてきた手法であり、**本章で取り上げるのもFirst-visit MC法である。**

Every-visit MC : 第9章と第12章で議論するように、Every-visit MCは関数近似と適格性トレースにより自然に拡張される。


---
<style scoped>
section { font-size: 20px; }
</style>

# First-visit MCのアルゴリズム : estimate $V \approx v_{\pi}$

Input: 
- 評価されるポリシー$\pi$ 

初期化:
- $V (s) \in R$、任意、すべての $s \in S$ に対して
- $Returns(s)$ ← すべての $s \in S$ に対して、空のリスト。

永遠にループする(各エピソードごとに):
- $\pi$に続くエピソードを生成する:$s_{0} , a_{0} , r_{1} , s_{1} , a_{1} , r_{2} ,..., s_{T—1} , a_{T—1} , r_{T}$ 
- $g ← 0$ 
- エピソードの各ステップ、t = T -1,T -2, ..., 0に対してループ: 
  - $G ← \gamma G + R_{t+1}$ ($\gamma$は割引率)
  - $S_{t}$が$S_{0}, S_{1} ,..., S_{t—1}$に出現しない限り:
    - $Returns(S_{t})$に$G$を追加する。
    - $V(S_{t}) ← average(Returns(S_{t}))$


---
# Example 5.1: Blackjack 
- 二枚のカードの合計が21に近い方が勝ち。21を超えると負け。21以下ならカードを引くか引かないか (hit or stick) を選択できる。
- 絵札は10, aceは1 or 11になる.
- **The game begins with two cards dealt to both dealer and player.**
  - **One of the dealer’s cards is face up and the other is face down.**
- ディーラーは、17以上になるまでカードを引く。17以上になったら、引かない。21を超えたら負け。21以下なら、プレイヤーと比較して勝ち負けが決まる。

---
# Example 5.1: Blackjack 
ブラックジャックをプレイすることは，当然ながらエピソード有限MDPとして定式化される. 
- Each game of blackjack is an episode. 
- Rewards of +1, -1, and 0 are given for winning, losing, and drawing, respectively. 
- All rewards within a game are zero, and we do not discount ($\gamma$ = 1); 
- therefore these terminal rewards are also the returns. 
- The player’s actions are to hit or to stick. The states depend on the player’s cards and the dealer’s showing card. 


--- 
# Example 5.1: Blackjack 
**We assume that cards are dealt from an infinite deck (i.e., with replacement) so that there is no advantage to keeping track of the cards already dealt.** ← 重要な前提条件(現実とは違う)。

もし、playerがaceを11として使えるなら、それをusableと定義する。usableなら、hitしたほうが絶対に良い(ace = 1 or 11)。

Thus, the player makes decisions on the basis of three variables: 
 - his current sum(12–21), 
 - the dealer’s one showing card (ace–10), 
 - and whether or not he holds a usable ace. 

This makes for a total of 200 states.

---
# Example 5.1: Blackjack 
プレイヤーが20か21であればstandし，そうでなければhitするというpolicyを考える.

**このポリシーのstate-value関数をモンテカルロ法で求めるには、このpolicyを使ってブラックジャックゲームをたくさんシミュレートし，各状態に続くリターンを平均化する.**

このようにして，図5.1に示す状態値関数の推定値が得られた (教科書参照).

---
# Example 5.1: Blackjack 
**ブラックジャックのタスクでは環境に関する完全な知識を持っているが、DP法を適用して価値関数を計算することは容易ではない。**

DP法は次のイベントの分布を必要とする。
- 例えば，プレイヤーが14で，スティック=standを選んだとする。ディーラーがカードを出したときの関数として，プレイヤーの報酬が+1される確率は何であろうか?

- DPを適用する前に、すべての確率を計算しなければならないが、このような計算は複雑でエラーが発生しやすいことが多い。

- 対照的に、モンテカルロ法で必要なサンプルゲームの生成は簡単である。これは驚くほどよくあることである。

---
# Example 5.1: Blackjack 
モンテカルロ法がサンプルエピソードだけで動作する能力は、環境のダイナミクスに関する完全な知識を持っている場合でも、 重要な利点となりうる。

モンテカルロ法の重要な点は、各状態の推定値が独立していることである。DPの場合のように、ある状態の推定値が他の状態の推定値の上に構築されることはない。

---
# 5.2 Estimation of Action Values (modelがない場合)

モデルがあれば、状態値だけでpolicyを決めるのに十分である。DPの章でやったように、一歩先を見て、報酬と次の状態の最良の組み合わせにつながる行動を選ぶだけである。

**しかし、モデルがなければ、状態値だけでは十分ではない。** 
  - その値をpolicy提案に役立てるために、各行動の値を明示的に推定する必要がある
  - したがって、モンテカルロ法の主要な目標の1つは、$q_{*}$を推定することである。
  - これを達成するために、まず、行動値に対するpolicyの評価問題を考える。

---
# 行動に対するpolicy評価の問題
行動に対するpolicy評価の問題 : 
- 状態$s$から始めて行動$a$をとり、その後policy$\pi$に従うときの期待リターン$q_{\pi}(s, a)$を推定すること。
- ただし、ここでは状態への訪問ではなく、状態とアクションのペアへの訪問について述べる。
- 状態-行動ペア$(s,a)$は、あるエピソードにおいて、状態$s$が訪問され、その中で行動$a$が取られた場合に、訪問されたという。

唯一の複雑な点は、多くの状態とアクションのペアが一度も訪問されない可能性があることである。

---
# 行動に対するpolicy評価の問題
もし$\pi$が 決定論的なpolicyであれば、$\pi$に従うと、各状態からのアクションのうち1つだけのリターンを観察することになる。平均するリターンがない場合、モンテカルロ法による他のアクションは経験によって向上することはない。

アクションの価値を学習する目的は、各状態で利用可能なアクションの中から選択するのに役立てることであるため、これは深刻な問題である。

選択肢を比較するためには、現在好んでいる行動だけでなく、各状態のすべての行動の価値を推定する必要がある。

そこで、エピソードが無限に続く限り、すべての状態アクション・ペアが無限に訪れることを保証する。これを探索開始の仮定と呼ぶ。

今のところ、探索開始の仮定を維持し、完全なモンテカルロ制御法の提示を完了する。

---
# 5.3 Monte Carlo Control
これで、モンテカルロ推定が制御においてどのように利用できるか、つまり、 最適なポリシーを近似することができるかについて考える準備ができた。

全体的な考え方は、DPの章と同じパターンの考え方に従って進めることである。

GPIでは、近似的な方針と近似的な価値関数の両方を維持する。
- 価値関数は現在の方針の価値関数により近くなるように繰り返し変更され、
- 方針は現在の価値関数に対して繰り返し改善される。

この2種類の変更を一緒に行うことで、政策と価値関数の両方が最適に近づきます。

---
# 5.3 Monte Carlo Control
はじめに、古典的な政策反復のモンテカルロ版を考えてみよう。この方法では、政策評価と政策反復の完全なステップを交互に実行する。任意のpolicy$\pi_{0}$から始まり、最適な政策と最適な行動価値関数で終わる:

$\pi_{0}\stackrel{E}{\longrightarrow}q_{\pi_{0}}\stackrel{I}{\longrightarrow}\pi_{1}\stackrel{E}{\longrightarrow}q_{\pi_{1}}\stackrel{I}{\longrightarrow}\pi_{2}\cdots$

ここで、
- $\stackrel{E}{\longrightarrow}$は完全なpolicy(complete policy)評価を表し、
- $\stackrel{I}{\longrightarrow}$は完全なpolicy improvementを表す。

---
# 5.3 Monte Carlo Control
policy評価は、前節で説明したとおりに行われる。

多くのエピソードが経験され、近似的な行動価値関数が漸近的に真の関数に近づく。

とりあえず、本当に無限のエピソードを観察し、さらに、エピソードが次のように生成されると仮定しよう。

これらの仮定の下で、モンテカルロ法は、任意の$q_{\pi_{k}}$に対して、各$\pi_{k}$を正確に計算する。


---
# 5.3 Monte Carlo Control
ポリシーの改善は、現在のポリシーを貪欲にすることによって行われる。

この場合、行動価値関数があるので、貪欲policyを構成するためのモデルは必要ない。

任意action-value関数$q$に対する貪欲な政策は、各$s \in S$に対して、決定論的に行動価値が最大の行動を選択するものである。

$\pi(s) \stackrel{\mathrm{def}}{=}	 \underset{a} {\argmax}$ $q(s, a)$ $\cdots$ (5.1) 

policy improvementは，各$\pi_{k+1}$ を $\pi_{q_{k}}$に関して貪欲な政策として構成することで可能である。

政策改善定理(セクション4.2)は⇡に適用される。k

---
と⇡k+1 なぜなら、すべてのs∈Sに対して、
q⇡ k (s, ⇡k+1 (s)) = q⇡ k (s, argmax q⇡ k (s, a))
a
=max q⇡ k (s, a) a
≥ q⇡k(s,⇡k(s)) ≥ v⇡k(s)。
前章で述べたように、定理は、各 ⇡k+1 が ⇡k よりも一様に優れているか、また は ⇡k と同じくらい優れていることを保証する。このことは、全体的なプロセス が最適政策と最適政策に収束することを保証する。


---
価値関数である。このようにして、モンテカルロ法を用いれば、サンプルエピ ソードだけで、環境のダイナミクスに関する他の知識がなくても、最適な方針 を見つけることができる。
このモンテカルロ法の収束保証を簡単に得るために、我々は上記のような2つ
のあり得ない仮定をした。一つは、エピソードが探索開始を持つことであり、
もう一つは、政策評価は無限のエピソード数で行うことができるということで
ある。実用的なアルゴリズムを得るためには、両方の仮定を取り除く必要があ
る。最初の仮定についての考察はこの章の後半まで延期する.

---
今のところ、政策評価は無限のエピソードで行われるという仮定に注目する 。この仮定を取り除くのは比較的簡単である.実際，反復的な政策評価のような 古典的なDP手法においても，同じ問題が生じ，それは真の価値関数に漸近的に のみ収束する.DPの場合もモンテカルロの場合も、問題を解決する方法は2つあ ります。一つは、各政策評価においてq⇡ kを近似するという考えを堅持するこ とである。推定値の誤差の大きさと確率の境界を得るために測定と仮定を行い 、各政策評価の間に十分なステップを踏んで、これらの境界が十分に小さいこ とを保証します。このアプローチは、ある程度の近似レベルまでは正しい収束 を保証するという意味で、おそらく完全に満足のいくものにできるだろう。し かし、最小の問題以外では、実際には有用でないほど多くのエピソードを必要 とする可能性が高い。

---
政策評価に必要な無限回数のエピソードを回避する第二のアプローチは、政 策改善に戻る前に政策評価を完了することをあきらめることである。各評価ス テップで価値関数を q⇡ k に近づけますが、多くのステップを経なければ実際に 近づくことは期待できません。セクション4.6でGPIの考え方を最初に紹介したと きに、この考え方を使いました。この考え方の一つの極端な形態は、値の反復 であり、この場合、ポリシーの改善の各ステップの間に、ポリシーの反復評価の 1つの反復のみが実行される。インプレース版の値反復はさらに極端で、1つの 状態に対して改善と評価のステップを交互に行います。

---
モンテカルロ・ポリシーの反復では、エピソードごとに評価と改善を交互 に行うのが自然である。各エピソードの後、観測されたリターンがポリシー の評価に使用され、その後、そのエピソードで訪れたすべての状態においてポリシーが改善される。モンテカルロES(Monte Carlo with Exploring Starts)と呼 ぶ、このような単純なアルゴリズムが、次ページのボックスに擬似コードで示 されています。

---
モンテカルロESでは、各状態とアクションのペアのすべてのリターンが、 それらが観察されたときにどのような政策が有効であったかに関係なく、累 積され、平均化される。モンテカルロESがどのような準最適政策にも収束で きないことは容易に理解できる。もし収束すれば、価値関数は最終的にその 政策の価値関数に収束し、その結果、政策が変更されることになる。安定は 、政策と価値関数の両方が最適である場合にのみ達成される。行動-価値関数 の変化が時間とともに減少するにつれて、この最適な定点に収束することは 避けられないように思われるが、まだ正式には証明されていない。我々の意 見では、これは強化学習における最も基本的な未解決の理論的問題の1つであ る(部分的な解答についてはTsitsiklis, 2002を参照)。


---
例5.3:ブラックジャックを解く ブラックジャックにモンテカルロESを適用するのは 簡単である.エピソードはすべてシミュレートされたゲームであるので，すべて の可能性を含む探索を始めるようにアレンジするのは簡単である.この場合，ディーラーのカード，プレイヤーの合計，そして，プレイヤーが使えるエース
を持っているかどうかを，すべて同じ確率でランダムに選ぶだけである.初期 方針として，前のブラックジャックの例で評価された方針，つまり，20か21に のみ固執する方針を使う.初期行動値関数は，すべての状態-行動ペアに対して 0にすることができる.図5.2は、モンテカルロESによって得られたブラックジ ャックの最適な方針を示している。この政策は，Thorp (1966)の「基本」戦略と 同じであるが，唯一，使えるエースに対する政策の左端の切り欠きは，Thorpの 戦略にはない.この不一致の理由は不明であるが，ここで示したものは，我々 が説明したブラックジャックのバージョンに最適な政策であると確信している
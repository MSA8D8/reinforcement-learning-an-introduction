---
marp: true
theme: default
math : MathJax


---
<!-- paginate: true -->
# Chapter 5: Monte Carlo Methods
##### 2023.12.1
##### 佐々木

<!-- <style scoped>
section { font-size: 23px; }
</style> -->

---
# Monte Carlo methodsの導入
- 本章では、価値関数を推定し、最適な方針を発見するための最初の学習法を考える。**前章とは異なり、ここでは環境の完全な知識を前提としない。**
- モンテカルロ法は、実際の、あるいはシミュレートされた環境から得られた、状態、行動、報酬のサンプルシーケンスである経験だけを必要とする。
- モデルは必要であるが、 **動的計画法(DP)に必要なすべての可能な遷移の完全な確率分布ではなく、サンプルの遷移を生成するだけでよい。**

---
# Monte Carlo methods 
モンテカルロという用語 : 操作に重要なランダム要素が含まれるあらゆる推定方法に対して、より広義に使用されることが多い。

ここでは特に、**完全なリターン(= complete return)の平均に基づく方法**(部分的なリターンから学習する方法は次の章で検討)に使用する。

- well-definedされたリターンが得られるように、**ここではモンテカルロ法をエピソード・タスクに対してのみ定義する。**
  - つまり、**経験はエピソードに分割され、どのような行動を選択しても、すべてのエピソードは最終的に終了すると仮定する。**
  - エピソードが終了したときのみ、valueのestimateとactionが変更される。
  - したがって、エピソードごとの意味ではインクリメンタルであるが、ステップバイステップ(オンライン)の意味ではインクリメンタルではない。

---
# Monte Carlo methods 
モンテカルロ法は、
- 各状態とアクションのペアのリターンをサンプリングして平均する。
- つまり、ある状態で行動をとった後のリターンは、同じエピソード内の後の状態でとった行動に依存する。
- よって、すべての行動選択は学習中であるため、先の状態から見て問題は非定常となる。

---
# Monte Carlo for episodic tasks
非定常性を扱うために、第4章で紹介した**GPI(general policy iteration)の考え方をDPに適用する。** 
  - 4章では、MDPの知識から価値関数を計算したが、ここではMDPを用いたサンプルリターンから価値関数を学習する。
  - 価値関数とそれに対応するpolicyは、基本的に同じ方法(GPI)で最適性を達成するために相互作用する。

DPの章と同様に、
- まず予測問題を考える (the computation of $v_{\pi}$ and $q_{\pi}$ for a fixed arbitrary policy $\pi$)
- 次に、policy imporovement
- 最後に、GPIによる制御問題とその解法。

DPからの各アイデアは、サンプル経験のみが利用可能なモンテカルロ法に拡張される

---
# 5.1 Monte Carlo Prediction
**まず、与えられたpolicyの状態-価値関数を学習するモンテカルロ法を考える。** 

- ある状態($s$)の価値($v$)とは、その状態からの期待リターン(将来の割引報酬の累積期待値)であることを思い出してください。

- 経験則からこの値を推定する明白な方法は、その状態を訪れた後に観察されたリターンを単純に平均すること。

- より多くのリターンが観測されれば、平均値は期待値に収束するはずである。

この考え方は、すべてのモンテカルロ法の根底にある。

---
# 5.1 Monte Carlo Prediction
**あるポリシー$\pi$に従って$s$を通過することによって得られるエピソードの集合が与えられたとき、ポリシーの下での状態$s$の値$v_{\pi}(s)$を推定したいとする。**

あるエピソードにおける状態$s$の各出現は、$s$への訪問(visit)と呼ばれる。もちろん、$s$は同じエピソードで複数回訪問されるかもしれない。あるエピソードで$s$が最初に訪問されたときを、$s$へのfirst-visitと呼ぶことにする。

- first-visit MCの場合、$v_{\pi}(s)$を$s$への初回訪問後のリターンの平均として推定する
- 一方、毎回訪問する(every-visit)場合は、$s$へのすべての訪問後のリターンを平均する

これら2つのモンテカルロ法は非常によく似ているが、理論的性質は若干異なる。**本章で取り上げるのもFirst-visit MC法である。**

---
<style scoped>
section { font-size: 20px; }
</style>

# First-visit MCのアルゴリズム : estimate $V \approx v_{\pi}$

Input: 
- 評価されるポリシー$\pi$ 

初期化:
- $V (s) \in R$、任意、すべての $s \in S$ に対して
- $Returns(s)$ ← すべての $s \in S$ に対して、空のリスト。

永遠にループする(各エピソードごとに):
- $\pi$に続くエピソードを生成する:$s_{0} , a_{0} , r_{1} , s_{1} , a_{1} , r_{2} ,..., s_{T—1} , a_{T—1} , r_{T}$ 
- $g ← 0$ 
- エピソードの各ステップ、t = T -1,T -2, ..., 0に対してループ: 
  - $G ← \gamma G + R_{t+1}$ ($\gamma$は割引率)
  - $S_{t}$が$S_{0}, S_{1} ,..., S_{t—1}$に出現しない限り:
    - $Returns(S_{t})$に$G$を追加する。
    - $V(S_{t}) ← average(Returns(S_{t}))$


---
# Example 5.1: Blackjack 
- 二枚のカードの合計が21に近い方が勝ち。21を超えると負け。21以下ならカードを引くか引かないか (hit or stick) を選択できる。
- 絵札は10, aceは1 or 11になる.
- **The game begins with two cards dealt to both dealer and player.**
  - **One of the dealer’s cards is face up and the other is face down.**
- ディーラーは、17以上になるまでカードを引く。17以上になったら、引かない。21を超えたら負け。21以下なら、プレイヤーと比較して勝ち負けが決まる。

---
# Example 5.1: Blackjack 
ブラックジャックをプレイすることは，当然ながらエピソード有限MDPとして定式化される. 
- Each game of blackjack is an episode. 
- Rewards of +1, -1, and 0 are given for winning, losing, and drawing, respectively. 
- All rewards within a game are zero, and we do not discount ($\gamma$ = 1); 
- therefore these terminal rewards are also the returns. 
- The player’s actions are to hit or to stick. The states depend on the player’s cards and the dealer’s showing card. 


--- 
# Example 5.1: Blackjack 
**We assume that cards are dealt from an infinite deck (i.e., with replacement) so that there is no advantage to keeping track of the cards already dealt.** ← 重要な前提条件(現実とは違う)。

もし、playerがaceを11として使えるなら、それをusableと定義する。usableなら、hitしたほうが絶対に良い(ace = 1 or 11)。

Thus, the player makes decisions on the basis of three variables: 
 - his current sum(12–21), 
 - the dealer’s one showing card (ace–10), 
 - and whether or not he holds a usable ace. 

This makes for a total of 200 states.

---
# Example 5.1: Blackjack 
プレイヤーが20か21であればstandし，そうでなければhitするというpolicyを考える.

**このポリシーのstate-value関数をモンテカルロ法で求めるには、このpolicyを使ってブラックジャックゲームをたくさんシミュレートし，各状態に続くリターンを平均化する.**

このようにして，図5.1に示す状態値関数の推定値が得られた (教科書参照).

---
# Example 5.1: Blackjack 
**ブラックジャックのタスクでは環境に関する完全な知識を持っているが、DP法を適用して価値関数を計算することは容易ではない。**
- 例えば，プレイヤーが14で，スティック=standを選んだとする。ディーラーがカードを出したときの関数として，プレイヤーの報酬が+1される確率は?

- DPを適用する前に、すべての確率を計算しなければならないが、このような計算は複雑でエラーが発生しやすいことが多い。対照的に、モンテカルロ法で必要なサンプルゲームの生成は簡単である。

- モンテカルロ法の重要な点は、各状態の推定値が独立していることである。DPの場合のように、ある状態の推定値が他の状態の推定値の上に構築されることはない。

---
# 5.2 Estimation of Action Values (modelがない場合)
モデルがあれば、状態値だけでpolicyを決めるのに十分である。DPの章でやったように、一歩先を見て、報酬と次の状態の最良の組み合わせにつながる行動を選ぶだけである。

**しかし、モデルがなければ、状態値だけでは十分ではない。** 
  - その値をpolicy提案に役立てるために、各行動の値を明示的に推定する必要がある
  - したがって、モンテカルロ法の主要な目標の1つは、$q_{*}$を推定することである。
  - これを達成するために、**まず、action-valueに対するpolicyの評価問題を考える。**

---
# action-valueに対するpolicy評価の問題
行動に対するpolicy評価の問題 : 
- 状態 $s$ から始めて行動 $a$ をとり、その後policy $\pi$ に従うときの期待リターン$q_{\pi}(s, a)$を推定すること。
- ただし、ここでは状態への訪問ではなく、状態とアクションのペアへの訪問について述べる。
- 状態-行動ペア$(s,a)$は、あるエピソードにおいて、状態$s$が訪問され、その中で行動$a$が取られた場合に、訪問されたという。

唯一の複雑な点は、**多くの状態とアクションのペアが一度も訪問されない可能性があることである。**

---
# 行動に対するpolicy評価の問題
もし平均するリターンがない場合、モンテカルロ法による他のアクションは経験によって向上することはない。これは深刻な問題である。選択肢を比較するためには、各状態のすべての行動の価値を推定する必要がある。

そこで、エピソードが無限に続く限り、すべての状態アクション・ペアが無限に訪れることを保証する。これを探索開始の仮定と呼ぶ。

今のところ、探索開始の仮定を維持し、完全なモンテカルロ制御法の提示を完了する。

---
# 5.3 Monte Carlo Control
これで、モンテカルロ推定から最適なポリシーを近似することについて考える準備ができた。

全体的な考え方は、DPの章と同じパターンの考え方に従って進めることである。

GPIでは、近似的な方針と近似的な価値関数の両方を維持する。
- 価値関数は現在の方針の価値関数により近くなるように繰り返し変更され、
- 方針は現在の価値関数に対して繰り返し改善される。

この2種類の変更を一緒に行うことで、政策と価値関数の両方が最適に近づく。

---
# 5.3 Monte Carlo Control
はじめに、古典的なpolicy反復を考えてみよう。この方法では、政策評価と政策反復の完全なステップを交互に実行する。任意のpolicy$\pi_{0}$から始まり、最適な政策と最適な行動価値関数で終わる:

$\pi_{0}\stackrel{E}{\longrightarrow}q_{\pi_{0}}\stackrel{I}{\longrightarrow}\pi_{1}\stackrel{E}{\longrightarrow}q_{\pi_{1}}\stackrel{I}{\longrightarrow}\pi_{2}\cdots$

ここで、
- $\stackrel{E}{\longrightarrow}$は完全なpolicy(complete policy)評価を表し、
- $\stackrel{I}{\longrightarrow}$は完全なpolicy improvementを表す。

とりあえず、本当に無限のエピソードを観察し、さらに、エピソードが次のように生成されると仮定しよう。これらの仮定の下で、モンテカルロ法は、任意の$q_{\pi_{k}}$に対して、各$\pi_{k}$を正確に計算する。

---
# 5.3 Monte Carlo Control
ポリシーの改善は、現在のポリシーを貪欲にすることによって行われる。

この場合、行動価値関数があるので、貪欲policyを構成するためのモデルは必要ない。

任意のaction-value関数$q$に対する貪欲な政策は、各$s \in S$に対して、決定論的に行動価値が最大の行動を選択するものである。

$\pi(s) \stackrel{\mathrm{def}}{=} \underset{a} {\argmax}$ $q(s, a)$ $\cdots$ (5.1) 

---
# 5.3 Monte Carlo Control
policy improvementは，各$\pi_{k+1}$を$q_{\pi_{k}}$に関して貪欲な政策として構成することで可能である。

policy improvement theorem(セクション4.2)は$\pi_{k}$と$\pi_{k+1}$に適用される。

なぜなら、すべての$s \in S$に対して、

$$q_{\pi_{k}}(s, \pi_{k+1}(s)) = q_{\pi_{k}}(s, \underset{a} {\argmax} q(s, a))$$
$$=\underset{a} {\max} q_{\pi_{k}} (s, a)$$
$$\geqq q_{\pi_{k}} (s, \pi_{k}(s))$$ 
$$\geqq v_{\pi_{k}}(s)$$

---
# 5.3 Monte Carlo Control
以上から、各$\pi_{k+1}$が$\pi_{k}$よりも一様に優れているか、または$\pi_k$と同じくらい優れていることを保証する。このようにして、サンプルエピソードだけで、最適なpolicyを見つけることができる。

**しかし、このモンテカルロ法の収束保証を簡単に得るために、我々は以下の2つのあり得ない仮定をした。**
- 一つは、エピソードが探索開始を持つことであり、
- もう一つは、policy評価は無限のエピソード数で行うことができるということである。

実用的なアルゴリズムを得るためには、両方の仮定を取り除く必要がある。

---
# 5.3 Monte Carlo Control (無限のエピソード数)
2つめの「policy評価は無限のエピソードで行われる」という、ありえない仮定に注目する。

この仮定を取り除くのは比較的簡単である。問題を解決する方法は2つあります。
1. 各policy評価において$q_{\pi_{k}}$を近似するという考えを堅持することである。
2. policy改善に戻る前にpolicy評価を完了することをあきらめることである。

---
# 1. 各policy評価において$q_{\pi_{k}}$を近似するという考えを堅持することである。
推定値の誤差の大きさと確率の境界を得るために測定と仮定を行い、各政策評価の間に十分なステップを踏んで、これらの境界が十分に小さいことを保証します。

このアプローチは、ある程度の近似レベルまでは正しい収束を保証するという意味で、おそらく完全に満足のいくものにできるだろう。

しかし、最小の問題以外では、実際には有用でないほど多くのエピソードを必要とする可能性が高い。

---
# 2. policy改善に戻る前にpolicy評価を完了することをあきらめることである。
各評価ステップで価値関数を$q_{\pi_{k}}$に近づけますが、多くのステップを経なければ実際に近づくことは期待できません。

この考え方の一つの極端な形態は、値の反復であり、この場合、ポリシーの改善の各ステップの間に、ポリシーの反復評価の1つの反復のみが実行される。インプレース版の値反復はさらに極端で、1つの状態に対して改善と評価のステップを交互に行います。

---
# 5.3 Monte Carlo Control (無限のエピソード数)
モンテカルロ・ポリシーの反復では、エピソードごとに評価と改善を交互に行うのが自然である。各エピソードの後、観測されたリターンがポリシーの評価に使用され、その後、そのエピソードで訪れたすべての状態においてポリシーが改善される。

モンテカルロES(Monte Carlo with Exploring Starts)と呼ぶ、このような単純なアルゴリズムが、次ページのボックスに擬似コードで示されています。

---
# モンテカルロES
モンテカルロESでは、各状態とアクションのペアのすべてのリターンが、それらが観察されたときにどのような政策が有効であったかに関係なく、累積され、平均化される。モンテカルロESがどのような準最適policyにも収束できないことは容易に理解できる。

もし収束すれば、価値関数は最終的にその政策の価値関数に収束し、その結果、政策が変更されることになる。安定は、政策と価値関数の両方が最適である場合にのみ達成される。

---
# 5.4 モンテカルロ・コントロール (探索なしスタート)
1つ目の「探索開始」というありえない仮定を避けるにはどうすればいいのだろうか?すべての行動が無限に選択されることを保証する唯一の一般的な方法は、エージェントが行動を選択し続けることである。

これを確実にするには2つのアプローチがあり、on policy法とoff policy法と呼ばれるものがある。
- on policy : 意思決定に使用されたポリシーを評価または改善する
- off policy : データを生成するために使用されたポリシーとは異なるポリシーを評価または改善する

上記で開発したモンテカルロES法はオンポリシー法の一例である。

本節では、探索開始という非現実的な仮定を用いないon policy モンテカルロ制御法を設計する方法を示す。

---
# 5.4 モンテカルロ・コントロール (探索なしスタート)
本節で紹介するon-policy法はε-greedy policyを用いる。つまり、ほとんどの場合、推定行動値が最大になる行動を選択するが、確率εで、代わりにランダムに行動を選択する。

つまり、すべての非貪欲アクションは最小の選択確率、ε/|A(s)|を与えられ、残りの大部分の確率、1 - ε + ε/|A(s)|が貪欲な行動に与えられる。

ε-貪欲な政策はε-ソフトポリシーの一例であり、ε - ソフト・ ポリシーのうち、ε - グリード・ポリシーとは、ある意味で最も貪欲に近いポリシーである。 

---
# 5.4 モンテカルロ・コントロール (探索なしスタート)
オンポリシーモンテカルロ制御の全体的な考え方は、やはりGPIのそれである。モンテカルロESと同様に、ファースト・ヴィジットMC法を用いて、現在のポリシーの行動価値関数を推定する。しかし、探索開始の仮定がなけ
れば、現在の価値関数に関して貪欲にすることで単純にポリシーを改善することはできません。

---
# 5.4 モンテカルロ・コントロール (探索なしスタート)
幸いなことに、GPIは、政策を貪欲な政策まで持っていくことを要求しない。
我々のオンポリシー法では、"貪欲なポリシー"にのみ移動させる。どのような"-soft policy, ⇡に対しても、q⇡に関するどのような"-greedy policyも、⇡以上になることが保証される。完全なアルゴリズムは下のボックスに示されている。


---
# 5.4 モンテカルロ・コントロール (探索なしスタート)
q⇡ に関する任意のε-greedy 方針が、任意のε-soft 方針πに対する改善であるこ
とは、 方針改善定理によって保証される。π′ をε-貪欲政策とする。政策改善定
理の条件が適用されるのは、任意の s∈S:

---
# 5.4 モンテカルロ・コントロール (探索なしスタート)
したがって、政策改善定理によって、π′ π≥(すなわち、≥すべてのs Sに対∈して、 v⇡ 0 (s) v⇡ (s))が成り立つ。ここで、π′ と π の両方がε-ソフト・ポリシーの中 で最適であるとき、つまり、他のすべてのε- ソ フ ト ・ ポリシーより優れている か等しいときのみ、等式が成り立つことを証明する。

---
# 5.4 モンテカルロ・コントロール (探索なしスタート)
ポリシーが環境の内部にε-soft「移動」されるという要件を除いて、元の環境と同じである新しい環境を考える。

新しい環境は元の環境と同じ行動と状態セットを持ち、次のように振る舞う。状態 s でアクション a を取る場合、確率 1-εで、新しい環境は正確に次のように振る舞う。

確率εで、ランダムに行動を選び直す。 そして、新しいランダムな行動で古い環境のように振る舞う。

この新しい環境において一般的な方針でできる最善は、元の環境においてε-ソフトな方針でできる最善と同じである

---
v* と q* を新しい環境に対する最適な価値関数とする。

すると、ある政策πはv*⇡ = v* の場合に限り、ε-ソフトe・ポリシーが適用eされる。 

遷移確率を変更したベルマン最適方程式(3.19):

しかし、この式は、vを代入する以外は、前の式と同じである。⇡
v が唯一の解であるため、v = v でなければならない。 **ee⇡*e
′*′
 A(s)| A(s)
等式が成り立ち、ε-ソフト・ポリシーπが改善されなくなるとき、(5.2)から次のこ
⇡ |A(s)| q⇡ a a
aXX ⇡)i + p(s′ ,r|s, a)hr + (s′ .
X
q一 ⇡

要するに、ε-ソフト・ポリシーに対してポリシーの反復が機能することをこ
5.4.モンテカルロ・コントロール(探索開始なし 103 こ数ページで示した。

---
ε-ソフトポリシーの貪欲ポリシーという自然な概念を用いると、ε-ソフトポリシーの中で最良のポリシーが発見されたときを除いて、すべてのステップで改善が保証される。この分析は、行動値関数がどのように決定されるかに依存しない

しかし、各段階で正確に計算されることを前提としている。これにより、前節 とほぼ同じことが言える。今、我々はε-ソフトポリシーの中で最良のポリシー
を達成するだけであるが、その一方で、以下の仮定を排除した。 探索が始まる。


---
# 5.5 重要度サンプリングによるOff政策予測
すべての学習制御法はジレンマに直面している:学習制御法は、その後の最適 行動を条件として行動値を学習しようとするが、すべての行動を探索する(最 適行動を見つける)ためには非最適行動をとる必要がある。探索的な方針に従 って行動しながら、最適な方針を学習するにはどうすればよいのだろうか?前 節のオン・ポリシー・アプローチは実際には妥協の産物であり、最適なポリシ ーではなく、探索を続ける最適に近いポリシーについて行動値を学習する。1つ は学習され、最適なポリシーとなり、もう1つはより探索的で、行動を生成する ために使用されます。学習されるポリシーをターゲット・ポリシーと呼び、行 動を生成するために使用されるポリシーをビヘイビア・ポリシーと呼ぶ。この 場合、学習はターゲットポリシーの「o↵」データから行われ、全体的なプロセ スはo↵ポリシー学習と呼ばれる。

---
本書の残りの部分では、オン↵ポリシー・メソッドとオン↵ポリシー・メソッ ドの両方を検討する。オン⇄ポリシー法は一般的に単純であり、最初に考察する 。O↵ポリシー法は、追加の概念と表記法を必要とし、データが異なるポリシー によるものであるため、O↵ポリシー法は多くの場合分散が大きく、収束が遅い 。一方、o↵ポリシー法はより強力で一般的である。オン↵ポリシー法は、ターゲ ットとビヘイビアポリシーが同じ場合の特別なケースである。O↵ポリシー法は また、アプリケーションにおいて様々な追加的な用途がある。例えば、従来の 非学習型コントローラによって生成されたデータや、人間の専門家によって生 成されたデータから学習するために適用できることが多い。O↵ポリシー学習は また、世界のダイナミクスの多段階予測モデルを学習するための鍵であると考 える人もいる(セクション17.2; Sutton, 2009; Sutton et al.)

---
このセクションでは、ターゲットと行動ポリシーの両方が固定された予測問 題を考えることによって、o↵ポリシー法の研究を開始する。すなわち，我々は v⇡ または q⇡ を推定したいが，我々が持っているのは別の方針bに従ったエピソードだけであるとする.

この場合、πはターゲット・ポリシー、bはビヘイビア・ポリシーであり、どちらのポリシーも固定され、与えられたものとみなされる。つまり、π(a s) > 0は、b(a s) > 0を意味する。これをカバレッジの仮定と呼ぶ。

一方、目標政策πは決定論的であってもよく、実際、これは制御の応用において特に興味深いケースである。制御では、目標政策は通常行動価値関数の現在の推定値に関する決定論的貪欲政策。

この政策が決定論的最適政策になる一方で、行動政策は確率的であり、より探索的である。しかし、この節では、πが不変であり、与 えられたものである予測問題を考える。

---

ほとんどすべての↵政策手法は、ある分布の下での期待値を別の分布からのサ ンプルで推定する一般的な手法である重要度サンプリングを利用している。我 々は重要度サンプリングをoȕ政策学習に適用し、重要度サンプリング比と呼ば れる、目標政策と行動政策の下で発生する軌道の相対確率に従ってリターンに
重み付けを行う。ある開始状態St が与えられたとき、その後の 状態-動作の軌跡、At , St+1 , At+1 , . . . , ST は、任意のポリシーπの下で発生する。
Pr{At ,St+1 ,At+1 ,..., ST |St ,At:T—1 ⇠π}.
= π(A |Stt )p(St+1 |St , At )π(A |St+1t+1 ) -- - p(ST |S T—1 , A T—1 )
T -1 Y
k=t
ここでpは(3.4)で定義される状態遷移確率関数である。従って、目標方針と行動 方針の下での軌道の相対確率(重要度サンプリング比)は
=
π(A |Skk )p(Sk+1 |Sk , Ak )、
T -1 Y
k= b(A|S t|S t)kk
QT -1
.k=tπ(A|S)k k)p(Sk+1|Sk,Ak)
π(A|S)
k k .
⇢t:T-1 = QT-1b(A = k= k k)p(Sk+1|SkA)k
(5.3)
軌跡確率は一般に未知であるMDPの遷移確率に依存するが、分子と分母の両方 に同じように現れるため、相殺される。重要度サンプリング比は結局、2つの政 策とシーケンスにのみ依存し、MDPには依存しない。
目標政策の下での期待リターン(値)を推定したいが、我々が持っているの は行動政策によるリターンGt だけであることを思い出してほしい。これらのリ
ターンは誤った
期待値E[G |Stt =s] = vb (s)であるため、平均してv⇡ を求めることはできない。 ここで重要度サンプリングの出番となる。比率 ⇢ t:T—1 はリターンが正しい期 待値を持つように変換する:
E[⇢t:T —1 Gt | St =s] = v⇡ (s). (5.4)
これで、政策bに従って観察されたエピソードのバッチからのリターンを平均 して、v⇡ (s)を推定するモンテカルロ・アルゴリズムを与える準備ができた。こ こでは，エピソードの境界を越えて増加するように時間ステップを番号付けす るのが便利である.すなわち，バッチの最初のエピソードが時間100の終端状態 で終了する場合，次のエピソードは時間t = 101で始まる.これにより、特定の エピソードの特定のステップを参照するために、タイム・ステップ番}号を使用 することができる。特に、状態sが訪問されるすべての時間ステップの集合を

T(s)と定義することができる。これはevery-visitメソッドの場合であり、first-visit
メソッドの場合、T(s)はエピソード内でsへの最初の訪問である時間ステップの みを含む。また、T(t)は時刻tに続く最初の終了時刻を表し、Gt は時刻tの後、
次の時刻まで戻ることを表す。
T(t)である。そして、{G }tt2T(s) は状態 s に関係するリターンであり、 ⇢ t:T(t)—1
t2T(s) は以下の通りである。
対応する重要度サンプリング比率。v⇡ (s) を推定するには、単純にリターンを
比率でスケーPリングし、結果を平均する:
t2T(s) ⇢t:T (t)-1Gt
V (s) = |T(s)|。 . 

このように重要度サンプリングが単純平均として行われる場合は、通常の重要度 サンプリングと呼ばれる。
重要な代替案は、重み付き重要度サンプリングである。 と定義される。
PP
. t2T(s) ρt:T (t)-1Gt V (s) =
t2T(s) ρt:T (t)-1
, (5.6)
分母がゼロの場合はゼロとなる。これらの2種類の重要度サンプリングを理解す るために、状態sからの単一のリターンを観察した後のファーストビジット法の 推定値を考えてみましょう。加重平均推定値では、単一のリターンに対する比 率ρ t:T(t)—1
は分子と分母で相殺され、推定値は観測値と等しくなる。 比率に依存しないリターン(比率がゼロでないと仮定)。このリターンが観
測された唯一のものであることを考えると，これは妥当な推定値ですが，その 期待値は v⇡ (s) ではなく vb (s) であり，この統計的な意味ではバイアスがか かっています.対照的に，通常の重要度サンプリング推定量(5.5)の初回訪問 バージョンは，期待値は常にv⇡ (s)である(不偏である)が，極端になるこ とがある.この比率が10であったとすると，観察された軌跡は，目標政策下 では行動政策下の10倍の可能性があることを示す.この場合、通常の重要度 サンプリングの推定値は、観察されたリターンの10倍となります。つまり、 そのエピソードの軌跡がターゲット・ポリシーの非常に代表的なものである と考えられるにもかかわらず、観測されたリターンからかなり離れてしまう のです。
形式的には，2種類の重要度サンプリングの初回訪問法の間の差↵は，それら の偏りと分散で表現される.通常の重要度サンプリングは不偏であるのに対し て、重み付き重要度サンプリングは偏りがある(ただし、偏りは漸近的にゼロ に収束する)。一方，通常の重要度サンプリングの分散は，一般に，比率の分 散が束縛されないので，束縛されませんが，重み付き推定では，単一のリター ンの最大の重みは1です.実際、リターンが有界であると仮定すると、比率自体 の分散が無限大であっても、重み付き重要度サンプリング推定量の分散はゼロ に収束します(Precup, Sutton, and Dasgupta 2001)。実際には，重み付き推定量は通 常，分散が劇的に小さく，強く好まれる。とはいえ，通常の重要度サンプリングは，本書の第2部で検討する関数近似を用いた近似手法に拡張する方が簡単で
あるため，完全に放棄するつもりはない. 通常の重要度サンプリングと重み付き重要度サンプリングのEvery-visit法は、
どちらも偏りがあるが、これもサンプル数が増えるにつれて偏りは漸近的に ゼロになる。実際には、どの状態が訪問されたかを追跡する必要がなく、ま た近似への拡張がより簡単であるため、every-visit法が好まれることが多い。 重み付き重要度サンプリングを用いたo↵ポリシー政策評価のための完全な every-visit MCアルゴリズムは、110ページの次のセクションで与えられる。
練習5.5 単一の非終端状態と、確率pで非終端状態に遷移し、確率1p で 終端状態 に遷移する単一のアクションを-持つMDPを考える。
ц = 1.10ステップ続く1つのエピソードを観察し、リターンが10であるとする 。非終端状態の値の初回訪問推定量と毎回訪問推定量は? ⇤

---
5.6 Incremental Implementation

モンテカルロ予測法は、第2章(セクション2.4)で説明したテクニックを拡張し たもので、エピソードごとに段階的に実装することができる。第 2 章では報酬 を平均化したが、モンテカルロ法ではリターンを平均化する。その他の点では 、第2章で使用したのとまったく同じ方法が、オン・ポリシー・モンテ・カルロ 法にも使用できる。o↵ポリシー・モンテカルロ法については、通常の重要度サ ンプリングを使用するものと、重み付き重要度サンプリングを使用するものを 分けて考える必要がある。
通常の重要度サンプリングでは、リターンは重要度サンプリング比ρ t:T(t)— 1 (5.3)でスケーリングされ、(5.5)のように単純に平均化される。これらの手 法では、第2章の漸増法を再び使うことができるが、スケーリングされたリ ターンの代わりに
はその章の報酬である。これは、重み付き重要度サンプリングを使用したo↵ポ リシー法の場合を残す。ここでは、リターンの加重平均を形成する必要があり 、少し異なるインクリメンタルアルゴリズムが必要となる。
G1 , G2 ,... , Gn—1 , すべてが同じ状態から始まり、それぞれが対応するラ ンダムなウェイト Wi (例えば、Wi = ρt i :T (ti )-1)を持つリターンのシーケン スがあるとする。我々は、次のような推定を行いたい。

nVn を追跡することに加えて、各状態について、最初のn個のリターンに与えら
—週 k=1
れた重みの累積和Cn を保持しなければならない。Vn の更新ルールは
n ≥ 1、
Vn+1 = V n そして
G n - Vn ,
(5.8)
. Wnh i
Cn+1 . =Cn + Wn+1、
Cn
.
ここで、C0 = 0(およびV1 は任意であるため、指定する必要はない)。上の ボックスは
次のページには、モンテカルロ・ポリシー評価のための完全なエピソードごと のインクリメンタル・アルゴリズムが含まれている。このアルゴリズムは，名

目上は，重み付き重要度サンプリングを用いたo↵ポリシーの場合のためのもの
ですが，次のように選択するだけで，on-ポリシーの場合にも適用できます. ターゲット・ポリシーとビヘイビア・ポリシーを同じにする(この場合(π=
b)、Wは常に1)。このとき Q の近似値は q⇡ に収束する。
は、潜在的に異なるポリシーbに従って選択される。


---
# 5.7 Offポリシー・モンテカルロ制御
ここで、本書で考察する2番目のクラスの学習制御法の例を紹介しよう。オンȕ ポリシー法の特徴は、ポリシーの値を推定しながら、それを制御に使うことで ある。oȕポリシー・メソッドでは、この2つの機能は分離されています。振る舞 いポリシーと呼ばれる、振る舞いを生成するために使用されるポリシーは、タ ーゲットポリシーと呼ばれる、評価され改善されるポリシーとは無関係である 可能性があります。この分離の利点は、ターゲット・ポリシーが決定論的(例 えば、貪欲)である可能性がある一方で、ビヘイビア・ポリシーがすべての可 能なアクションをサンプリングし続けることができることで あ る 。
O↵政策モンテカルロ制御法は、前の2つのセクションで紹介した手法のい ずれかを使用する。モンテカルロ制御法は、ターゲット・ポリシーを学習し 改善しながら、行動ポリシーに従う。これらの技法は、行動方針が、目標方 針によって選択される可能性のある全ての行動を選択する確率が0でないこ と(カバレッジ)を必要とする。全ての可能性を探索するために、行動方針 がソフトであること(すなわち、全ての状態において全ての行動をゼロでな い確率で選択すること)を要求する。
次ページのボックスは、GPI と重み付き重要度サンプリングに基づく、π* と
q* を推定するための o↵ ポリシー・モンテカルロ制御法を示しています。 目標 ≈ポリシー π π* は、q⇡ の推定値である Q に関する貪欲なポリシーです。
政策bはどのようなものでも良いが、πが最適政策に収束することを保証するためには、πが最 適政策に収束するような政策が必要である。
各状態とアクションのペアに対して、無限のリターンを得なければならない。 これはbをε-softに選ぶことで保証できる。政策πは、行動ȕが異なるソフト政策b にしたがって選択されても、遭遇するすべての状態において最適に収束する。
エピソード間、あるいはエピソード内でも変化する。潜在的な問題は、この方法がエピソードの最後尾からしか学習しないという
ことである。もし貪欲でない行動が一般的であれば、特に長いエピソードの初
期に現れる状態の学習は遅くなる。これは学習を大幅に遅らせる可能性がある
。この問題がどの程度深刻かを評価するには、オフポリシー・モンテカルロ法
の経験が不十分である。この問題が深刻であるとすれば、最も重要な対処法は
、おそらく時間的モンテカルロ法を取り入れることであろう。
差分学習は、次章で開発されるアルゴリズムのアイデアである。あるいは、ц が1より小さい場合、次のセクションで開発されたアイデアも大いに役立つだろ う。
